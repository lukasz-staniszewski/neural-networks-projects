{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Wstęp do przetwarzania języka naturalnego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x27eb7b02450>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") \n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "+ Tworzony jest embedding ze wszystkich słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"https://galera.ii.pw.edu.pl/~kdeja/data/sst2.tsv\",delimiter=\"\\t\",quoting=3).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                            sentence  label\n0       hide new secretions from the parental units       0\n1               contains no wit , only labored gags       0\n2  that loves its characters and communicates som...      1\n3  remains utterly satisfied to remain the same t...      0\n4  on the worst revenge-of-the-nerds clichés the ...      0\n5  that 's far too tragic to merit such superfici...      0\n6  demonstrates that the director of such hollywo...      1\n7                                          of saucy       1\n8   a depressed fifteen-year-old 's suicidal poetry       0\n9  are more deeply thought through than in most `...      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>hide new secretions from the parental units</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>contains no wit , only labored gags</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>that loves its characters and communicates som...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>remains utterly satisfied to remain the same t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>that 's far too tragic to merit such superfici...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>demonstrates that the director of such hollywo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>of saucy</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>a depressed fifteen-year-old 's suicidal poetry</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>are more deeply thought through than in most `...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \n"
     ]
    }
   ],
   "source": [
    "print(reviews[\"sentence\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\01149762\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Można poprzerabiać opinie różnymi operacjami na słowach - przykladowo usuwajac myslniki pomiedzy:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    \"\"\"Function to convert a review to a string of words.\n",
    "    The input is a single string (a raw movie review), and the output is a single string (a preprocessed movie review)\"\"\"\n",
    "    review_text = BeautifulSoup(raw_review, 'lxml').get_text()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words('english'))\n",
    "    meaningful_words = [word for word in words if not word in stops]\n",
    "    return \" \".join(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst revenge nerds clich filmmakers could dredge\n"
     ]
    }
   ],
   "source": [
    "clean_review = review_to_words(reviews['sentence'][4])\n",
    "print(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 67349\n",
      "Review 2000 of 67349\n",
      "Review 3000 of 67349\n",
      "Review 4000 of 67349\n",
      "Review 5000 of 67349\n",
      "Review 6000 of 67349\n",
      "Review 7000 of 67349\n",
      "Review 8000 of 67349\n",
      "Review 9000 of 67349\n",
      "Review 10000 of 67349\n",
      "Review 11000 of 67349\n",
      "Review 12000 of 67349\n",
      "Review 13000 of 67349\n",
      "Review 14000 of 67349\n",
      "Review 15000 of 67349\n",
      "Review 16000 of 67349\n",
      "Review 17000 of 67349\n",
      "Review 18000 of 67349\n",
      "Review 19000 of 67349\n",
      "Review 20000 of 67349\n",
      "Review 21000 of 67349\n",
      "Review 22000 of 67349\n",
      "Review 23000 of 67349\n",
      "Review 24000 of 67349\n",
      "Review 25000 of 67349\n",
      "Review 26000 of 67349\n",
      "Review 27000 of 67349\n",
      "Review 28000 of 67349\n",
      "Review 29000 of 67349\n",
      "Review 30000 of 67349\n",
      "Review 31000 of 67349\n",
      "Review 32000 of 67349\n",
      "Review 33000 of 67349\n",
      "Review 34000 of 67349\n",
      "Review 35000 of 67349\n",
      "Review 36000 of 67349\n",
      "Review 37000 of 67349\n",
      "Review 38000 of 67349\n",
      "Review 39000 of 67349\n",
      "Review 40000 of 67349\n",
      "Review 41000 of 67349\n",
      "Review 42000 of 67349\n",
      "Review 43000 of 67349\n",
      "Review 44000 of 67349\n",
      "Review 45000 of 67349\n",
      "Review 46000 of 67349\n",
      "Review 47000 of 67349\n",
      "Review 48000 of 67349\n",
      "Review 49000 of 67349\n",
      "Review 50000 of 67349\n",
      "Review 51000 of 67349\n",
      "Review 52000 of 67349\n",
      "Review 53000 of 67349\n",
      "Review 54000 of 67349\n",
      "Review 55000 of 67349\n",
      "Review 56000 of 67349\n",
      "Review 57000 of 67349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\01149762\\documents\\ai\\neural-networks-intro\\venv\\lib\\site-packages\\bs4\\__init__.py:346: MarkupResemblesLocatorWarning: \"con \" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 58000 of 67349\n",
      "Review 59000 of 67349\n",
      "Review 60000 of 67349\n",
      "Review 61000 of 67349\n",
      "Review 62000 of 67349\n",
      "Review 63000 of 67349\n",
      "Review 64000 of 67349\n",
      "Review 65000 of 67349\n",
      "Review 66000 of 67349\n",
      "Review 67000 of 67349\n"
     ]
    }
   ],
   "source": [
    "# creating list of cleaned sentences\n",
    "\n",
    "num_reviews = reviews['sentence'].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length of the move review list\n",
    "for review in range(0, num_reviews):\n",
    "    # If the index is evenly divisible by 100, print a message\n",
    "    if (review+1) % 1000 == 0:\n",
    "        print('Review {} of {}'.format(review+1, num_reviews))\n",
    "    # Call our function for each one, and add the result to the list of clean reviews\n",
    "    clean_train_reviews.append(review_to_words(reviews['sentence'][review]))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Teraz tworzymy Bag of Words:\n",
    "+ Sklearn ma przyjemny intefejs do tego\n",
    "+ Wszystkie recenzje bierze i przerabia tak ze tworzy slownik i dla kazdej recenzji przypisuje 1 temu słowu co wystepuje w danej recenzji\n",
    "+ Max_features - jaki maksymalnie moze byc ten słownik."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "Bag of words completed\n"
     ]
    }
   ],
   "source": [
    "print('Creating the bag of words...')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                            tokenizer = None,\n",
    "                            preprocessor = None,\n",
    "                            stop_words = None,\n",
    "                            max_features = 1000)\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocaulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "train_data_features = train_data_features.toarray()\n",
    "print('Bag of words completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\01149762\\documents\\ai\\neural-networks-intro\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.random.rand(len(reviews))>0.3\n",
    "train_data = torch.from_numpy(train_data_features).float()[train_indices]\n",
    "train_targets = torch.from_numpy(reviews[\"label\"].values[train_indices]).long()\n",
    "\n",
    "test_data = torch.from_numpy(train_data_features[~train_indices]).float()\n",
    "test_targets = torch.from_numpy(reviews[\"label\"].values[~train_indices]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.TensorDataset(train_data,train_targets)\n",
    "test_dataset = data.TensorDataset(test_data,test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n         1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n         0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0])]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Klasyfikator na tym działający:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "BoWClassifier(\n  (lin1): Linear(in_features=1000, out_features=500, bias=True)\n  (act1): LeakyReLU(negative_slope=0.01)\n  (lin2): Linear(in_features=500, out_features=50, bias=True)\n  (act2): LeakyReLU(negative_slope=0.01)\n  (lin3): Linear(in_features=50, out_features=5, bias=True)\n)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.lin1 =nn.Linear(1000, 500)  # 28 x 28 = 784\n",
    "        self.act1 =nn.LeakyReLU()\n",
    "        self.lin2 =nn.Linear(500, 50)\n",
    "        self.act2 =nn.LeakyReLU()\n",
    "        self.lin3 =nn.Linear(50, 5)\n",
    "        \n",
    "             \n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "bow_model = BoWClassifier().to(device)\n",
    "bow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval() #*********#\n",
    "    for imgs, labels in data_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        output = model(imgs)\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max logit\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += imgs.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 0.545 test_acc: 0.757\n",
      "Epoch 1 loss 0.444 test_acc: 0.771\n",
      "Epoch 2 loss 0.408 test_acc: 0.785\n",
      "Epoch 3 loss 0.372 test_acc: 0.796\n",
      "Epoch 4 loss 0.336 test_acc: 0.802\n",
      "Epoch 5 loss 0.307 test_acc: 0.81\n",
      "Epoch 6 loss 0.288 test_acc: 0.81\n",
      "Epoch 7 loss 0.274 test_acc: 0.815\n",
      "Epoch 8 loss 0.263 test_acc: 0.816\n",
      "Epoch 9 loss 0.257 test_acc: 0.814\n",
      "Final Training Accuracy: 0.861333446866485\n",
      "Final Validation Accuracy: 0.8143686073957513\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(bow_model.parameters(), lr=0.001)\n",
    "\n",
    "iters = []\n",
    "losses = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "for n in range(10):\n",
    "    epoch_losses = []\n",
    "    for x, labels in iter(train_loader):\n",
    "        x, labels = x.to(device), labels.to(device)\n",
    "        bow_model.train() \n",
    "        out = bow_model(x).squeeze()           \n",
    "\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()  \n",
    "        epoch_losses.append(loss.item())\n",
    "        optimizer.step()              \n",
    "        optimizer.zero_grad()         \n",
    "\n",
    "    loss_mean = np.array(epoch_losses).mean()\n",
    "    iters.append(n)\n",
    "    losses.append(loss_mean)\n",
    "    test_acc = get_accuracy(bow_model, test_loader)\n",
    "    print(f\"Epoch {n} loss {loss_mean:.3} test_acc: {test_acc:.3}\")\n",
    "    train_acc.append(get_accuracy(bow_model, train_loader)) # compute training accuracy \n",
    "    val_acc.append(test_acc)  # compute validation accuracy\n",
    "        \n",
    "\n",
    "print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
    "print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Problem - mamy dwa zdania skrajne, jednak podobne w budowie:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  1.4038,   2.1177, -11.4292, -11.3506, -12.1349],\n        [  1.4038,   2.1177, -11.4292, -11.3506, -12.1349]], device='cuda:0',\n       grad_fn=<AddmmBackward>)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_1_text = \"I do not like this movie\"\n",
    "example_2_text = \"I like this movie\"\n",
    "examples = vectorizer.transform([review_to_words(example_1_text),review_to_words(example_2_text)])\n",
    "examples = torch.from_numpy(examples.toarray()).to(device).float()\n",
    "bow_model(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dostajemy oba prawdopodobieństwa takie same.\n",
    "\n",
    "Tak samo inne zdania ale mające te same słówka - nie bierzemy pod uwagę kolejności, ale to czy występuje dane słowo, a ważna jest kolejność i kontekst w zdaniu."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0.4295,   4.4130, -15.6650, -15.5892, -16.6678],\n        [  0.4295,   4.4130, -15.6650, -15.5892, -16.6678]], device='cuda:0',\n       grad_fn=<AddmmBackward>)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_1_text = \"The topic of this movie is love\"\n",
    "example_2_text = \"I love a movie about this topic\"\n",
    "examples = vectorizer.transform([review_to_words(example_1_text),review_to_words(example_2_text)])\n",
    "examples = torch.from_numpy(examples.toarray()).to(device).float()\n",
    "bow_model(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddingi w języku\n",
    "Żeby sobie z tym radzić, musimy mądrzej zamienić słówka na wektory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "corpus = api.load('text8')\n",
    "gensim_model = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Umiejscowienie słowa w przestrzeni ukrytej."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 9.9947846e-01,  2.0548723e+00,  2.7577671e-01,  1.9341202e+00,\n       -6.5007991e-01,  2.4704773e+00, -3.3740111e-02,  2.5519125e+00,\n       -9.6115583e-01,  1.2598658e+00, -1.3981167e+00, -2.5443034e+00,\n       -4.7923112e-01,  5.1033940e+00, -8.9924759e-01, -8.1812471e-01,\n        7.6209313e-01, -1.6932794e-01,  3.4893429e+00, -1.2106495e+00,\n       -1.8109655e-03,  1.4378246e+00, -2.1039746e+00,  2.9412648e-01,\n       -6.7178959e-01,  2.3759677e+00,  4.9908015e-01, -1.4430287e+00,\n        1.7050871e+00, -5.6682903e-01, -1.5091728e+00, -2.4103981e-01,\n       -8.1385720e-01,  1.8808872e-01,  1.9908564e+00,  8.6683130e-01,\n        2.0249300e+00, -2.0167792e-02, -3.3703586e-01, -1.7170844e+00,\n        1.2650793e+00,  2.4709014e-02,  2.6755352e+00, -5.5685073e-01,\n       -2.2047420e+00, -3.2267447e+00,  1.1880048e-01, -1.4540140e-01,\n        3.7892771e+00, -1.7120577e+00,  3.3106608e+00,  1.0479050e+00,\n       -2.2053809e+00,  1.8163931e+00,  8.2389069e-01, -2.6601384e+00,\n       -4.3603274e-01, -3.2989871e-02,  6.7233801e-01, -1.8436570e+00,\n        5.2795309e-01, -1.0270107e+00,  2.1155083e+00, -3.6624732e+00,\n        4.5352051e-01, -4.3973893e-01, -2.5946283e+00, -2.6187864e-01,\n        9.3446004e-01, -3.9804172e-02,  2.8306240e-01,  4.7154952e-02,\n       -2.6700988e+00, -1.8709542e+00,  3.4870997e-01, -8.6881542e-01,\n        1.1816097e+00,  3.1755483e+00, -3.4035182e+00,  3.1376274e+00,\n       -3.3648336e+00, -2.2636855e+00,  5.1484776e-01,  4.2773966e-02,\n       -6.5387565e-01,  1.3934088e-01,  1.1935781e+00,  6.9799267e-02,\n       -1.5791107e+00, -7.3256916e-01, -6.0993284e-01, -1.1998986e+00,\n        8.4311467e-01,  8.5684896e-02,  2.0190978e+00,  4.4703901e-01,\n        1.6861233e+00,  1.0525609e+00, -5.0729066e-01, -5.5281534e-03],\n      dtype=float32)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv[\"king\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Najpopularniejsze słowa:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('prince', 0.758901834487915),\n ('emperor', 0.722903847694397),\n ('queen', 0.7127947807312012),\n ('kings', 0.7036997675895691),\n ('vii', 0.7014347314834595),\n ('throne', 0.6934410333633423),\n ('regent', 0.688907265663147),\n ('constantine', 0.6842485666275024),\n ('pharaoh', 0.6751309037208557),\n ('burgundy', 0.6630397439002991)]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('driver', 0.815956711769104),\n ('motorcycle', 0.7214707136154175),\n ('truck', 0.7195844650268555),\n ('cars', 0.7169750928878784),\n ('taxi', 0.6917212605476379),\n ('automobile', 0.6613979935646057),\n ('racing', 0.6593102812767029),\n ('vehicle', 0.6549674272537231),\n ('cab', 0.6481115221977234),\n ('passenger', 0.6480912566184998)]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv.most_similar(\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('loving', 0.6935794353485107),\n ('passion', 0.6466044187545776),\n ('affection', 0.6419485211372375),\n ('me', 0.6391317248344421),\n ('grace', 0.637935996055603),\n ('thee', 0.6365374326705933),\n ('praise', 0.6355740427970886),\n ('dreams', 0.6281015276908875),\n ('tragedy', 0.6041068434715271),\n ('soul', 0.6021668314933777)]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv.most_similar(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jak trenować embeddingi\n",
    "Jest gotowa warstwa - słowa można przetworzyć na indeksy i wrzucać te indeksy do Embedding i dostawać na wyjściu Embeddingi, które potem podajemy.\n",
    "Embedding to taki look-up table (LUT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3430,  0.8749,  1.8676, -0.1178,  0.0717]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Musimy wykorzystać to, że jakieś słowa muszą być podobne do innych. Słowa najczęściej definiuje ich otoczenie w tekście."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Bag-of-Words\n",
    "\n",
    "Przewidywanie słowa na podstawie kontekstu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'about', 'to', 'study', 'the', 'idea', 'of', 'a', 'computational', 'process.', 'computational', 'processes', 'are', 'abstract', 'beings', 'that', 'inhabit', 'computers.', 'as']\n",
      "[(['are', 'we', 'to', 'study'], 'about'), (['about', 'are', 'study', 'the'], 'to'), (['to', 'about', 'the', 'idea'], 'study')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2 # 2 words behind and 2 words after\n",
    "EMBEDDING_DIM = 10\n",
    "test_sentence = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".lower().split()\n",
    "\n",
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)] + [test_sentence[i+  j + 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence)-CONTEXT_SIZE)\n",
    "]\n",
    "# Print the first 3, just so you can see what they look like.\n",
    "print(test_sentence[:20])\n",
    "print(ngrams[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mamy metodę zamieniającą słowo na indeks w słowniku."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "{'that': 0,\n 'computational': 1,\n 'effect,': 2,\n 'abstract': 3,\n 'is': 4,\n 'our': 5,\n 'are': 6,\n 'people': 7,\n 'computers.': 8,\n 'called': 9,\n 'manipulate': 10,\n 'pattern': 11,\n 'as': 12,\n 'process.': 13,\n 'process': 14,\n 'program.': 15,\n 'idea': 16,\n 'the': 17,\n 'create': 18,\n 'evolution': 19,\n 'we': 20,\n 'computer': 21,\n 'spells.': 22,\n 'things': 23,\n 'of': 24,\n 'rules': 25,\n 'beings': 26,\n 'processes.': 27,\n 'they': 28,\n 'direct': 29,\n 'processes': 30,\n 'evolve,': 31,\n 'to': 32,\n 'a': 33,\n 'about': 34,\n 'in': 35,\n 'programs': 36,\n 'directed': 37,\n 'conjure': 38,\n 'other': 39,\n 'with': 40,\n 'study': 41,\n 'spirits': 42,\n 'data.': 43,\n 'inhabit': 44,\n 'by': 45}"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)               # index of word for a vecotr word\n",
    "        self.linear1 = nn.Linear(2* context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)                               # output is size of dictionary\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nauka:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226.27292656898499\n",
      "183.72114086151123\n",
      "145.24328649044037\n",
      "104.68410497903824\n",
      "67.49471786618233\n",
      "39.397829204797745\n",
      "22.243105672299862\n",
      "13.250607658177614\n",
      "8.575846806168556\n",
      "6.010881708934903\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()                                                    # oczekuje na wejsciu wektorów 0 i 1\n",
    "emb_model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.Adam(emb_model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "\n",
    "        # Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        emb_model.zero_grad()\n",
    "        log_probs = emb_model(context_idxs)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(total_loss)\n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Porównanie:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4980, -0.4948,  0.7871, -0.4056,  0.0681, -0.9177, -0.4468,  1.1779,\n",
      "         1.7163, -0.0595], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(emb_model.embeddings.weight[word_to_ix[\"computer\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7467, -1.0792, -0.1856,  1.5764,  0.2587, -1.5822, -1.0576, -2.2820,\n",
      "         2.1390, -0.7910], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(emb_model.embeddings.weight[word_to_ix[\"computational\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tak naprawdę potrzebujemy wytrenować model na ogromnym fragmencie tekstu - wtedy by to lepiej działało. Sprawdźmy jak podobne są wektory:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0530])\n",
      "tensor([-0.5541])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sim1 = torch.cosine_similarity(emb_model.embeddings.weight[word_to_ix[\"process\"]].unsqueeze(0),emb_model.embeddings.weight[word_to_ix[\"computational\"]].unsqueeze(0))\n",
    "    sim2 = torch.cosine_similarity(emb_model.embeddings.weight[word_to_ix[\"process\"]].unsqueeze(0),emb_model.embeddings.weight[word_to_ix[\"study\"]].unsqueeze(0))\n",
    "\n",
    "print(sim1)\n",
    "print(sim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bardziej podobne są process i study niż process i computational."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Przewidzenie słów których nie mieliśmy tu wcześniej - wystąpi błąd:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Śpiulkolot'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9448/3591153915.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0memb_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membeddings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mword_to_ix\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"Śpiulkolot\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m: 'Śpiulkolot'"
     ]
    }
   ],
   "source": [
    "print(emb_model.embeddings.weight[word_to_ix[\"Śpiulkolot\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([46, 10])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.embeddings.weight.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram - w odwrotną stronę\n",
    "Przewidujmy kontekst w oparciu o jedno słowo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'about', 'to', 'study', 'the', 'idea', 'of', 'a', 'computational', 'process.', 'computational', 'processes', 'are', 'abstract', 'beings', 'that', 'inhabit', 'computers.', 'as']\n",
      "[('about', ['are', 'we', 'to', 'study']), ('to', ['about', 'are', 'study', 'the']), ('study', ['to', 'about', 'the', 'idea'])]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2 # 2 words behind and 2 words after\n",
    "EMBEDDING_DIM = 10\n",
    "test_sentence = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".lower().split()\n",
    "\n",
    "ngrams = [\n",
    "    (\n",
    "        test_sentence[i],\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)] + [test_sentence[i+  j + 1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence)-CONTEXT_SIZE)\n",
    "]\n",
    "# Print the first 3, just so you can see what they look like.\n",
    "print(test_sentence[:20])\n",
    "print(ngrams[:3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "{'that': 0,\n 'computational': 1,\n 'effect,': 2,\n 'abstract': 3,\n 'is': 4,\n 'our': 5,\n 'are': 6,\n 'people': 7,\n 'computers.': 8,\n 'called': 9,\n 'manipulate': 10,\n 'pattern': 11,\n 'as': 12,\n 'process.': 13,\n 'process': 14,\n 'program.': 15,\n 'idea': 16,\n 'the': 17,\n 'create': 18,\n 'evolution': 19,\n 'we': 20,\n 'computer': 21,\n 'spells.': 22,\n 'things': 23,\n 'of': 24,\n 'rules': 25,\n 'beings': 26,\n 'processes.': 27,\n 'they': 28,\n 'direct': 29,\n 'processes': 30,\n 'evolve,': 31,\n 'to': 32,\n 'a': 33,\n 'about': 34,\n 'in': 35,\n 'programs': 36,\n 'directed': 37,\n 'conjure': 38,\n 'other': 39,\n 'with': 40,\n 'study': 41,\n 'spirits': 42,\n 'data.': 43,\n 'inhabit': 44,\n 'by': 45}"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)               # index of word for a vecotr word\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size * 2 * context_size)                               # output is size of dictionary\n",
    "        # self.linear2 = nn.Linear(128, embedding_dim)                               # output is size of dictionary\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1).view(2*self.context_size, -1)\n",
    "        # log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306.7320203781128\n",
      "273.11930084228516\n",
      "245.50446701049805\n",
      "219.04257345199585\n",
      "194.9033374786377\n",
      "174.08833122253418\n",
      "157.5209300518036\n",
      "145.15266382694244\n",
      "136.2757453918457\n",
      "129.94110238552094\n",
      "125.32269811630249\n",
      "121.8453516960144\n",
      "119.16303849220276\n",
      "117.05807077884674\n",
      "115.36728346347809\n",
      "113.97380578517914\n",
      "112.8363299369812\n",
      "111.87812340259552\n",
      "111.07376635074615\n",
      "110.40012323856354\n",
      "109.81551623344421\n",
      "109.36525189876556\n",
      "109.0983716249466\n",
      "108.80010414123535\n",
      "108.29901242256165\n",
      "108.07004296779633\n",
      "108.11904048919678\n",
      "108.292644739151\n",
      "108.59086298942566\n",
      "108.61948764324188\n",
      "108.37261319160461\n",
      "108.35817885398865\n",
      "108.01367259025574\n",
      "108.21533417701721\n",
      "108.00038242340088\n",
      "107.75092303752899\n",
      "107.53948533535004\n",
      "107.3577606678009\n",
      "107.09647107124329\n",
      "106.95789647102356\n",
      "107.04033064842224\n",
      "106.91067206859589\n",
      "106.84515178203583\n",
      "106.71501398086548\n",
      "106.58936131000519\n",
      "106.6052131652832\n",
      "106.4899480342865\n",
      "106.65316092967987\n",
      "106.53326785564423\n",
      "106.48586678504944\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()                                                    # oczekuje na wejsciu wektorów 0 i 1\n",
    "# loss_function = nn.BCEWithLogitsLoss()                                                    # oczekuje na wejsciu wektorów 0 i 1\n",
    "emb_model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.Adam(emb_model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    for target, context in ngrams:\n",
    "\n",
    "        # Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[target]], dtype=torch.long)\n",
    "        emb_model.zero_grad()\n",
    "        log_probs = emb_model(context_idxs)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[w] for w in context], dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(total_loss)\n",
    "    losses.append(total_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6074,  1.0224,  1.8924,  0.8053, -2.2798,  0.8791,  1.2199, -0.9913,\n",
      "        -0.2075,  0.0427], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(emb_model.embeddings.weight[word_to_ix[\"computer\"]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6966, -0.0856,  0.6345, -0.5837,  1.0689,  0.9280,  1.2877, -2.0443,\n",
      "         0.9691, -0.6113], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(emb_model.embeddings.weight[word_to_ix[\"computational\"]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5549])\n",
      "tensor([-0.1521])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sim1 = torch.cosine_similarity(emb_model.embeddings.weight[word_to_ix[\"process\"]].unsqueeze(0),emb_model.embeddings.weight[word_to_ix[\"computational\"]].unsqueeze(0))\n",
    "    sim2 = torch.cosine_similarity(emb_model.embeddings.weight[word_to_ix[\"process\"]].unsqueeze(0),emb_model.embeddings.weight[word_to_ix[\"study\"]].unsqueeze(0))\n",
    "\n",
    "print(sim1)\n",
    "print(sim2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Teraz trochę inne wyniki."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rozwiązywanie problemów z wykorzystaniem embeddingów\n",
    "Co daje wykorzystywanie wytrenowanych modeli.\n",
    "Weźmy te dobre embeddingi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_weights = torch.FloatTensor(gensim_model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([71290, 100])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_weights.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Słownik ma 70000 słów. Wykorzystajmy te embeddingi które już zostały nauczone i załadujmy je do modelu - w Torch prosta metoda."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding.from_pretrained(emb_weights)\n",
    "embedding.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Musimy pomapować słowa na indeksy - będziemy teraz operować na tzw. tokenach."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = gensim_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zamieniamy ciąg słów na ciąg tokenów aby te tokeny podawać na wejścia.\n",
    "Przechodzimy po słowach i do listy wkładamy zamienione na tokeny słowa."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_reviews_tokenized = []\n",
    "for review in reviews['sentence']:\n",
    "    unknows = 0\n",
    "    all_parsed = 0\n",
    "    review_tokenized = []\n",
    "    for word in review.split():\n",
    "        all_parsed+=1\n",
    "        try:\n",
    "            review_tokenized.append(tokenizer[word.lower()])\n",
    "        except:\n",
    "            unknows +=1\n",
    "#     print(unknows/all_parsed)\n",
    "    clean_train_reviews_tokenized.append(review_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sporo słów nie występuje w słowniku - jest to normalna sytuacja. Tokeny teraz poskładamy do pipeline."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, data,labels):\n",
    "        self.data = []\n",
    "        for d, l in zip(data,labels):\n",
    "            self.data.append((torch.from_numpy(np.array(d)).long(),torch.tensor(l).long()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_data, target = self.data[idx]\n",
    "        return in_data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ReviewDataset(np.array(clean_train_reviews_tokenized, dtype=object)[train_indices],reviews[\"label\"].values[train_indices])\n",
    "test_data = ReviewDataset(np.array(clean_train_reviews_tokenized, dtype=object)[~train_indices],reviews[\"label\"].values[~train_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tekst to sekwencja, a recenzje różnej długości - musimy dodać padding, żeby przetwarzać sensownie."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x)-1 for x in xx]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "    yy = torch.stack(yy)\n",
    "    return xx_pad, yy, x_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, collate_fn=pad_collate, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, collate_fn=pad_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model - korzystamy z sieci rekurencyjnej, przez którą przepuszczamy warstwy embeddingu i przetwarzamy dalej do predykcji.\n",
    "\n",
    "Tzn przetwarzamy wejściowe tokeny na embeddingi, przepuszczamy przez LSTM i na końcu klasyfikujemy biorąc pod uwagę ostatni element z sekwencji."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "LSTMRegressor(\n  (embeddings): Embedding(71290, 100)\n  (lstm): LSTM(100, 100)\n  (fc): Linear(in_features=100, out_features=5, bias=True)\n)"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "class LSTMRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, out_size, emb_weights, bidirectional = False):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        if bidirectional:\n",
    "            self.bidirectional = 2\n",
    "        else:\n",
    "            self.bidirectional = 1\n",
    "        # we wont train embeddings, because they are ok\n",
    "        self.embeddings = nn.Embedding.from_pretrained(emb_weights)\n",
    "        self.embeddings.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional=bidirectional, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_size*self.bidirectional, out_size)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
    "        state = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
    "        return hidden, state\n",
    "    \n",
    "    def forward(self, x, len_x, hidden):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.transpose(x,0,1)\n",
    "        all_outputs, hidden = self.lstm(x, hidden)\n",
    "        all_outputs = torch.transpose(all_outputs,0,1)\n",
    "        last_seq_items = all_outputs[range(all_outputs.shape[0]), len_x]\n",
    "        out = last_seq_items#all_outputs[-1]#torch.flatten(all_outputs,1)\n",
    "        x = self.fc(out)\n",
    "        return x, hidden\n",
    "     \n",
    "lstm_model = LSTMRegressor(100, 100, 1, 5, emb_weights).to(device)\n",
    "lstm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "I nauka:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.59\n",
      "Epoch: 10, loss: 0.156\n",
      "Epoch: 20, loss: 0.101\n",
      "Epoch: 30, loss: 0.0827\n",
      "Epoch: 40, loss: 0.0704\n",
      "Epoch: 50, loss: 0.0688\n",
      "Epoch: 60, loss: 0.0646\n",
      "Epoch: 70, loss: 0.058\n",
      "Epoch: 80, loss: 0.0581\n",
      "Epoch: 90, loss: 0.0531\n",
      "Epoch: 100, loss: 0.0526\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr = 0.001)\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "lstm_model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(101):\n",
    "    losses = 0\n",
    "    batches = 0\n",
    "    for x, targets, len_x in train_loader:\n",
    "        x = x.to(device)\n",
    "        targets = targets.to(device)\n",
    "        hidden, state = lstm_model.init_hidden(x.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device) \n",
    "        preds, _ = lstm_model(x, len_x, (hidden,state))\n",
    "        preds = preds.squeeze(1)\n",
    "        optimizer.zero_grad() \n",
    "        loss = loss_fun(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        batches +=1\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {losses/batches:.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_model.load_state_dict(torch.load(\"lab_13/lstm_model_dict\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    for x, targets, len_x in test_loader:\n",
    "        x = x.to(device)\n",
    "        targets_list.append(targets.numpy())\n",
    "        targets = targets.to(device)\n",
    "        hidden, state = lstm_model.init_hidden(x.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device) \n",
    "        preds, _ = lstm_model(x, len_x, (hidden,state))\n",
    "        preds = preds.squeeze(1)\n",
    "        preds_list.append(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.864\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test accuracy: {(np.argmax((np.concatenate(preds_list)),1) == np.concatenate(targets_list)).sum()/len(np.concatenate(targets_list)):.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Teraz wynik dobry."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model.state_dict(),\"models/lstm_model_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1_text = \"I do not like this movie\"\n",
    "example_2_text = \"I like this movie\"\n",
    "example_1_tokenized = []\n",
    "for word in example_1_text.split():\n",
    "    try:\n",
    "        example_1_tokenized.append(tokenizer[word])\n",
    "    except:\n",
    "        continue\n",
    "example_2_tokenized = []\n",
    "for word in example_2_text.split():\n",
    "    try:\n",
    "        example_2_tokenized.append(tokenizer[word])\n",
    "    except:\n",
    "        continue\n",
    "hidden, state = lstm_model.init_hidden(1)\n",
    "hidden, state = hidden.to(device), state.to(device) \n",
    "preds_1,_ = lstm_model(torch.from_numpy(np.array(example_1_tokenized)).unsqueeze(0).to(device),len(example_1_tokenized)-1,(hidden,state))\n",
    "preds_2,_ = lstm_model(torch.from_numpy(np.array(example_2_tokenized)).unsqueeze(0).to(device),len(example_2_tokenized)-1,(hidden,state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  4.9031,  -0.6317, -43.0387, -42.9470, -42.3877]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ -0.7449,   4.9343, -43.2633, -43.1928, -42.6310]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(preds_1)\n",
    "print(preds_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arytmetyka na embeddingach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.8411235 , -0.77230257,  1.8357811 ,  1.2937571 ,  0.6108541 ,\n       -0.07288974, -1.0088053 ,  0.06583275,  0.7922793 , -0.68797135,\n        1.8195764 ,  0.36700243,  0.5661307 ,  0.80233806, -2.6846557 ,\n       -0.46946242,  1.6577457 , -0.21098635,  0.37620822,  0.5768223 ,\n        1.5078772 ,  1.2165096 ,  1.6484568 ,  0.9887946 , -0.3875341 ,\n        0.02209038, -0.14449044,  1.597604  , -1.0570134 ,  0.9342247 ,\n        0.6880421 ,  2.1311483 ,  0.32119483,  0.92779136, -2.027819  ,\n       -1.0878956 , -1.6649443 , -2.628606  , -1.1843368 ,  0.15997778,\n        1.473094  , -1.1409794 , -0.04380501,  0.9330895 ,  1.1382651 ,\n       -1.3794839 , -2.138475  ,  0.11531267,  0.67792916, -0.7788164 ,\n       -0.6687108 , -0.01536954,  2.932167  , -1.3112794 ,  0.5719075 ,\n        2.2462325 , -0.54970056, -0.96162903,  0.42806396,  0.21307887,\n       -0.49051175,  0.83036745,  0.18435518,  1.0261692 , -1.1026707 ,\n       -1.2648658 , -0.61703414,  0.6436122 , -0.08553598, -1.7918525 ,\n        2.9346204 , -1.5902766 ,  0.7447911 ,  1.5000472 ,  2.3602006 ,\n        2.0651739 , -0.5999068 ,  1.7659656 , -2.5469553 , -0.20438732,\n        0.23385389, -2.6133397 ,  1.2046636 ,  1.2416345 , -1.8395101 ,\n       -0.1383069 ,  1.4763368 , -1.1051321 ,  0.89542824,  1.1383111 ,\n        1.3238722 , -2.1772738 , -2.5270197 ,  0.5443432 ,  0.27953333,\n       -2.3073318 ,  0.66504174, -1.3913023 ,  2.0875275 ,  2.0617185 ],\n      dtype=float32)"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv[\"car\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "982"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer[\"car\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.8411, -0.7723,  1.8358,  1.2938,  0.6109, -0.0729, -1.0088,  0.0658,\n         0.7923, -0.6880,  1.8196,  0.3670,  0.5661,  0.8023, -2.6847, -0.4695,\n         1.6577, -0.2110,  0.3762,  0.5768,  1.5079,  1.2165,  1.6485,  0.9888,\n        -0.3875,  0.0221, -0.1445,  1.5976, -1.0570,  0.9342,  0.6880,  2.1311,\n         0.3212,  0.9278, -2.0278, -1.0879, -1.6649, -2.6286, -1.1843,  0.1600,\n         1.4731, -1.1410, -0.0438,  0.9331,  1.1383, -1.3795, -2.1385,  0.1153,\n         0.6779, -0.7788, -0.6687, -0.0154,  2.9322, -1.3113,  0.5719,  2.2462,\n        -0.5497, -0.9616,  0.4281,  0.2131, -0.4905,  0.8304,  0.1844,  1.0262,\n        -1.1027, -1.2649, -0.6170,  0.6436, -0.0855, -1.7919,  2.9346, -1.5903,\n         0.7448,  1.5000,  2.3602,  2.0652, -0.5999,  1.7660, -2.5470, -0.2044,\n         0.2339, -2.6133,  1.2047,  1.2416, -1.8395, -0.1383,  1.4763, -1.1051,\n         0.8954,  1.1383,  1.3239, -2.1773, -2.5270,  0.5443,  0.2795, -2.3073,\n         0.6650, -1.3913,  2.0875,  2.0617])"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_weights[tokenizer[\"car\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mamy sporo podobieństwa między wektorami, gdy np. weźmiemy króla, odejmiemy od niego mężczyznę i dodamy kobietę, to otrzymamy coś podobnego do królowej - wyniki są w jakiś sposób podobne."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1080x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "sns.heatmap([gensim_model.wv[\"king\"], \n",
    "             gensim_model.wv[\"man\"], \n",
    "             gensim_model.wv[\"woman\"], \n",
    "             gensim_model.wv[\"king\"] - gensim_model.wv[\"man\"] + gensim_model.wv[\"woman\"],\n",
    "             gensim_model.wv[\"queen\"],\n",
    "            ], cbar=True, xticklabels=False, yticklabels=False,linewidths=1,cmap=\"vlag\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gensim_model.wv[\"paris\"] + gensim_model.wv[\"germany\"] - gensim_model.wv[\"berlin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([-5.95839620e-01,  4.83884156e-01,  1.79438281e+00,  1.43223405e+00,\n        1.12962079e+00,  7.42231727e-01, -2.45055389e+00,  2.71275330e+00,\n        2.52985263e+00,  3.39324212e+00, -6.09547615e-01,  4.19723332e-01,\n       -6.88719988e-01, -9.58087683e-01,  9.77741718e-01,  1.58371150e-01,\n        2.18805432e-01, -1.34157276e+00, -1.06756687e-01, -1.67740417e+00,\n        2.61722064e+00,  3.20417881e+00,  2.24567842e+00,  8.34653497e-01,\n       -5.83064556e-02,  2.81899977e+00, -2.26603508e-01, -3.61150384e-01,\n        1.21304154e-01,  7.04795420e-01, -6.28932357e-01,  3.70419770e-01,\n       -9.14212883e-01,  2.32326746e+00, -8.85619104e-01,  1.01870596e+00,\n        1.65169168e+00, -2.98628592e+00,  5.48151731e-01, -1.37352204e+00,\n        8.87057483e-01,  9.42169845e-01, -1.02907872e+00, -2.01587105e+00,\n       -1.29438090e+00, -3.13464493e-01,  2.95051694e+00,  2.89554596e+00,\n        9.09931004e-01,  1.53222859e-01,  1.25289309e+00,  1.72404301e+00,\n        5.71276307e-01,  9.72942829e-01,  3.53829169e+00, -3.94846201e-02,\n        7.93112636e-01,  1.22690654e+00,  5.29037714e-02, -2.50136209e+00,\n        1.20382977e+00,  1.31083071e+00, -7.11376429e-01, -6.11787915e-01,\n       -2.41436660e-01, -8.62498939e-01,  2.22186613e+00,  2.66430426e+00,\n        7.45404720e-01,  1.23312175e-01, -1.89565599e-01,  2.59976101e+00,\n       -2.24945307e+00, -3.64903212e-02,  1.74662411e+00, -4.84169722e-02,\n        9.74861622e-01,  2.70933414e+00,  6.09415770e-02,  5.64739704e-01,\n       -1.85281944e+00, -2.66504169e-01,  1.19193518e+00,  2.37879872e+00,\n        1.54192889e+00,  3.35013151e-01,  5.07367277e+00, -5.43908358e-01,\n       -1.91329980e+00,  1.23612976e+00,  8.46633315e-01, -2.18359089e+00,\n       -6.79919899e-01, -3.44531870e+00,  2.56394219e+00, -1.01416183e+00,\n        1.67310238e-04, -9.68044996e-01,  1.33564067e+00,  1.18949533e+00],\n      dtype=float32)"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.3716324 ,  2.0865693 ,  1.6778361 , -0.14426947,  1.1930861 ,\n        2.155661  , -0.49152768,  3.3454578 ,  0.7220797 ,  2.5041733 ,\n        0.62010866, -0.20087433, -1.8150067 ,  0.34477505,  1.2554781 ,\n        1.3071592 , -0.00776817, -0.17219262, -1.0021654 , -2.6784484 ,\n        1.4356128 ,  3.397555  ,  0.6945748 ,  0.5485612 , -1.123159  ,\n        1.8503331 , -0.7007616 ,  0.04393266,  1.4597993 , -0.5673457 ,\n       -0.3475152 ,  0.7696329 , -0.35259143,  2.071179  , -0.47037306,\n        0.6710733 ,  0.96819574, -1.4421403 ,  0.2369265 ,  0.2261117 ,\n        0.4931761 , -0.25110716,  0.04797141, -1.4399406 , -0.38885406,\n        1.7724222 ,  2.096675  ,  1.8491864 ,  1.692483  , -0.50916046,\n        2.1083694 ,  1.1956376 ,  0.45574528,  1.4536197 ,  2.0768542 ,\n       -0.22332563,  0.87108415,  0.99711066,  0.9323479 , -1.8658422 ,\n       -0.76421624,  0.22506371,  0.19454223, -1.2773122 ,  0.3590033 ,\n       -0.7415218 ,  0.8044129 ,  2.5632713 , -0.31483012, -0.47752285,\n       -1.5305073 ,  2.4846659 , -2.3607624 , -0.7131562 ,  1.3966285 ,\n       -0.10527974, -1.3535684 ,  1.8770245 , -0.34618217,  2.2694614 ,\n       -0.36757421,  0.4475139 ,  1.5828544 ,  2.2016191 ,  2.355284  ,\n       -0.03029019,  3.2059798 , -0.5812752 , -1.0788162 , -0.8012888 ,\n        0.5801202 , -1.4958667 , -0.16519248, -1.8736584 ,  2.3595166 ,\n       -0.8984394 , -0.16103126, -0.43467626, -0.29288483, -0.24807405],\n      dtype=float32)"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv[\"france\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jak możemy znaleźć do czego odnosi się wektor x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "keys = gensim_model.wv.index_to_key\n",
    "\n",
    "best_index = np.argmin(cdist(x.reshape(1,100), gensim_model.wv.vectors, \"cosine\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "'france'"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys[best_index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "[('france', 0.7936767935752869),\n ('italy', 0.7704543471336365),\n ('paris', 0.7676132321357727),\n ('germany', 0.7597668170928955),\n ('switzerland', 0.7032967209815979),\n ('belgium', 0.6825010776519775),\n ('spain', 0.656050980091095),\n ('austria', 0.6485854387283325),\n ('munich', 0.6217860579490662),\n ('vienna', 0.615205705165863)]"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv.most_similar(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Najmniej pasujące:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "'pineapple'"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv.doesnt_match(\"pizza tomato cheese pineapple\".split())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "'tree'"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv.doesnt_match(\"car bike boat tree\".split())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zaawansowane modele językowe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import *\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bert\n",
    "Zajmujemy się tłumaczeniem jednej sekwencji w drugą sekwencję.\n",
    "![title](https://i.pinimg.com/originals/d6/6a/3e/d66a3e867580854200fa37f08e8addaa.gif \"segment\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertModel(\n  (embeddings): Embeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (transformer): Transformer(\n    (layer): ModuleList(\n      (0): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (1): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (2): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (3): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (4): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (5): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Przetrenowane modele językowe:\n",
    "+ Huggingface: https://huggingface.co/transformers/pretrained_models.html\n",
    "+ Community: https://huggingface.co/models\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 14324, 2003, 2881, 2000, 3653, 1011, 3345, 2784, 7226, 7442, 7542, 2389, 15066, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = tokenizer(\"BERT is designed to pre-train deep bidirectional representations\")\n",
    "tokenized_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]',\n 'bert',\n 'is',\n 'designed',\n 'to',\n 'pre',\n '-',\n 'train',\n 'deep',\n 'bid',\n '##ire',\n '##ction',\n '##al',\n 'representations',\n '[SEP]']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_sentence[\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jedno słówko, jeśli go nie ma w słowniku, podzielił na kilka podsłówek. Tak więc słownik teraz ma fragmenty. Jest to dobre np w języku polskim."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 11867, 17922, 13687, 12898, 2102, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Śpiulkolot\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]', 'sp', '##iu', '##lk', '##olo', '##t', '[SEP]']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([101, 11867, 17922, 13687, 12898, 2102, 102])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dotrenowywanie gotowego modelu\n",
    "#### Załadowanie\n",
    "Przepuszczamy przez tokenizer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "odict_keys(['last_hidden_state'])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model(**tokenizer(\"BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.\", return_tensors=\"pt\"))\n",
    "result.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[-0.4867, -0.1770, -0.1706,  ..., -0.0752, -0.0195,  0.6925],\n          [ 0.0744, -0.2203,  0.1330,  ..., -0.0730, -0.1128,  0.3024],\n          [-0.6035, -0.2432, -0.0209,  ..., -0.2963, -0.6380,  0.6463],\n          ...,\n          [ 0.2347,  0.1218,  0.1038,  ..., -0.2239, -0.1634,  0.2523],\n          [ 0.7587,  0.1439, -0.5647,  ...,  0.4042, -0.7747, -0.2532],\n          [ 0.3978,  0.4038, -0.3577,  ...,  0.3608, -0.9130, -0.0176]]],\n        grad_fn=<NativeLayerNormBackward>),\n torch.Size([1, 28, 768]))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.last_hidden_state, result.last_hidden_state.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aby wziać gotowy model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Dotrenowywanie\n",
    "\n",
    "Biblioteka datasets - są tam popularne zbiory. Pozwala załadować zbiory.\n",
    "\n",
    "https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\01149762\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6bd1168b049842cb8b9cf702a9b5afbe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\",\"sst2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 67349\n    })\n    validation: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 872\n    })\n    test: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 1821\n    })\n})"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "['hide new secretions from the parental units ',\n 'contains no wit , only labored gags ',\n 'that loves its characters and communicates something rather beautiful about human nature ',\n 'remains utterly satisfied to remain the same throughout ',\n 'on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ',\n \"that 's far too tragic to merit such superficial treatment \",\n 'demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . ',\n 'of saucy ',\n \"a depressed fifteen-year-old 's suicidal poetry \",\n \"are more deeply thought through than in most ` right-thinking ' films \",\n 'goes to absurd lengths ',\n \"for those moviegoers who complain that ` they do n't make movies like they used to anymore \",\n \"the part where nothing 's happening , \",\n 'saw how bad this movie was ',\n 'lend some dignity to a dumb story ',\n 'the greatest musicians ',\n 'cold movie ',\n 'with his usual intelligence and subtlety ',\n 'redundant concept ',\n \"swimming is above all about a young woman 's face , and by casting an actress whose face projects that woman 's doubts and yearnings , it succeeds . \",\n 'equals the original and in some ways even betters it ',\n 'if anything , see it for karen black , who camps up a storm as a fringe feminist conspiracy theorist named dirty dick . ',\n 'a smile on your face ',\n 'comes from the brave , uninhibited performances ',\n 'excruciatingly unfunny and pitifully unromantic ',\n 'enriched by an imaginatively mixed cast of antic spirits ',\n \"which half of dragonfly is worse : the part where nothing 's happening , or the part where something 's happening \",\n 'in world cinema ',\n 'very good viewing alternative ',\n 'the plot is nothing but boilerplate clichés from start to finish , ',\n 'the action is stilted ',\n 'on all cylinders ',\n 'will find little of interest in this film , which is often preachy and poorly acted ',\n 'by far the worst movie of the year ',\n 'sit through , ',\n \"more than another `` best man '' clone by weaving a theme throughout this funny film \",\n \"it 's about issues most adults have to face in marriage and i think that 's what i liked about it -- the real issues tucked between the silly and crude storyline \",\n 'heroes ',\n 'oblivious to the existence of this film ',\n 'sharply ',\n 'the entire point of a shaggy dog story , of course , is that it goes nowhere , and this is classic nowheresville in every sense . ',\n 'sometimes dry ',\n \"as they come , already having been recycled more times than i 'd care to count \",\n 'covers this territory with wit and originality , suggesting that with his fourth feature ',\n 'a $ 40 million version of a game ',\n 'gorgeous and deceptively minimalist ',\n 'cross swords with the best of them and ',\n 'as a fringe feminist conspiracy theorist ',\n \"proves once again he has n't lost his touch , bringing off a superb performance in an admittedly middling film . \",\n 'disappointments ',\n 'the horrors ',\n 'a muddle splashed with bloody beauty as vivid as any scorsese has ever given us . ',\n 'many pointless ',\n 'a beautifully ',\n 'contrived , well-worn situations ',\n 'a doa ',\n \"poor ben bratt could n't find stardom if mapquest emailed him point-to-point driving directions . \",\n \"to be as subtle and touching as the son 's room \",\n 'starts with a legend ',\n 'far less sophisticated and ',\n 'rich veins of funny stuff in this movie ',\n 'no apparent joy ',\n 'shot on ugly digital video ',\n \"... a sour little movie at its core ; an exploration of the emptiness that underlay the relentless gaiety of the 1920 's ... the film 's ending has a `` what was it all for ? '' \",\n 'though ford and neeson capably hold our interest , but its just not a thrilling movie ',\n 'is pretty damned funny . ',\n 'we never feel anything for these characters ',\n \"'s a lousy one at that \",\n 'the corporate circus that is the recording industry in the current climate of mergers and downsizing ',\n 'the storylines are woven together skilfully , the magnificent swooping aerial shots are breathtaking , and the overall experience is awesome . ',\n 'of the most highly-praised disappointments i ',\n 'sounds like a cruel deception carried out by men of marginal intelligence , with reactionary ideas about women and a total lack of empathy . ',\n 'seem fresh ',\n 'to the dustbin of history ',\n 'as a director , eastwood is off his game ',\n 'pays earnest homage to turntablists ',\n 'weak and ',\n 'skip this dreck , ',\n 'contains very few laughs and even less surprises ',\n \"film to affirm love 's power to help people endure almost unimaginable horror \",\n 'are an absolute joy ',\n 'generates ',\n \", like life , is n't much fun without the highs and lows \",\n 'based on a true and historically significant story ',\n 'well-rounded tribute ',\n \", though many of the actors throw off a spark or two when they first appear , they ca n't generate enough heat in this cold vacuum of a comedy to start a reaction . \",\n 'so much like a young robert deniro ',\n 'khouri manages , with terrific flair , to keep the extremes of screwball farce and blood-curdling family intensity on one continuum . ',\n 'fashioning an engrossing entertainment out ',\n 'spiffy animated feature ',\n \"that 's so sloppily written and cast that you can not believe anyone more central to the creation of bugsy than the caterer \",\n 'alternating between facetious comic parody and pulp melodrama , this smart-aleck movie ... tosses around some intriguing questions about the difference between human and android life ',\n 'strung-together moments ',\n ', generous and subversive artworks ',\n \"it does n't follow the stale , standard , connect-the-dots storyline which has become commonplace in movies that explore the seamy underbelly of the criminal world \",\n 'funny yet ',\n 'overbearing and over-the-top ',\n \"it 's robert duvall ! \",\n 'rich and sudden wisdom ',\n \"acted and directed , it 's clear that washington most certainly has a new career ahead of him \",\n 'in memory ',\n 'respectable new one ',\n 'yet this grating showcase ',\n 'hate to tear your eyes away from the images long enough to read the subtitles ',\n 'addition to sporting one of the worst titles in recent cinematic history ',\n ', this gender-bending comedy is generally quite funny . ',\n \"build some robots , haul 'em to the theater with you for the late show , and put on your own mystery science theatre 3000 tribute to what is almost certainly going to go down as the worst -- and only -- killer website movie of this or any other year \",\n \"do n't work in concert \",\n 'the direction has a fluid , no-nonsense authority , and the performances by harris , phifer and cam ` ron seal the deal . ',\n 'would have liked it more if it had just gone that one step further ',\n \"it 's too harsh to work as a piece of storytelling , \",\n 'hawaiian shirt ',\n 'takes a classic story , casts attractive and talented actors and uses a magnificent landscape to create a feature film that is wickedly fun to watch . ',\n 'provide its keenest pleasures ',\n 'altogether too slight to be called any kind of masterpiece ',\n 'grievous but ',\n 'after you laugh once ( maybe twice ) , you will have completely forgotten the movie by the time you get back to your car in the parking lot . ',\n 'hopeless ',\n 'unpretentious , charming , quirky , original ',\n \"nicks and steinberg match their own creations for pure venality -- that 's giving it the old college try . \",\n 'very well-written and very well-acted . ',\n \"are n't many conclusive answers in the film \",\n 'clumsy dialogue , heavy-handed phoney-feeling sentiment , ',\n 'proves a lovely trifle that , unfortunately , is a little too in love with its own cuteness . ',\n 'bring tissues . ',\n \"the film 's mid-to-low budget is betrayed by the surprisingly shoddy makeup work . \",\n 'brings the proper conviction to his role as ( jason bourne ) . ',\n 'as a young woman of great charm , generosity and diplomacy ',\n \"there 's something poignant about an artist of 90-plus years taking the effort to share his impressions of life and loss and time and art with us . \",\n 'just too silly ',\n 'cinematic bon bons ',\n 'it is supremely unfunny and unentertaining to watch middle-age and ',\n 'a lively and engaging examination of how similar obsessions can dominate a family . ',\n 'irritates and ',\n 'collapse ',\n 'so many of the challenges it poses for itself that one can forgive the film its flaws ',\n \", it serves as a workable primer for the region 's recent history , and would make a terrific 10th-grade learning tool . \",\n 'wide-awake all the way through ',\n '( vainly , i think ) ',\n 'is one big excuse to play one lewd scene after another . ',\n 'is effective if you stick with it ',\n 'that underscore the importance of family tradition and familial community ',\n \"delivers what it promises : a look at the `` wild ride '' that ensues when brash young men set out to conquer the online world with laptops , cell phones and sketchy business plans \",\n 'watching this digital-effects-heavy , supposed family-friendly comedy ',\n 'absolutely and completely ridiculous and ',\n \"'s a very tasteful rock and roll movie . \",\n 'the movie is almost completely lacking in suspense , surprise and consistent emotional conviction . ',\n 'realistic portrayal ',\n 'as if trying to grab a lump of play-doh , the harder that liman tries to squeeze his story ',\n 'well-thought stunts or ',\n 'reveals how important our special talents can be when put in service of of others . ',\n 'no lika da ',\n 'a welcome relief ',\n 'the promise of digital filmmaking ',\n 'just one that could easily wait for your pay per view dollar ',\n 'justify a theatrical simulation of the death camp of auschwitz ii-birkenau ',\n 'extraordinary faith ',\n 'have i seen a film so willing to champion the fallibility of the human heart ',\n ', sometimes beautiful adaptation ',\n 'about as interesting ',\n 'caruso sometimes descends into sub-tarantino cuteness ... but for the most part he makes sure the salton sea works the way a good noir should , keeping it tight and nasty . ',\n 'overlong , and bombastic ',\n 'derivative and hammily ',\n 'most audacious , outrageous , ',\n 'at its best moments ',\n 'disney again ransacks its archives for a quick-buck sequel . ',\n 'that final , beautiful scene ',\n 'some movies suck you in despite their flaws , ',\n 'eats , meddles , argues , laughs , kibbitzes and fights ',\n 'the problem with the film is whether these ambitions , laudable in themselves , justify a theatrical simulation of the death camp of auschwitz ii-birkenau . ',\n ', compelling ',\n 'infectiously ',\n \"the picture runs a mere 84 minutes , but it 's no glance . \",\n 'think of this dog of a movie ',\n \"'s remarkable procession of sweeping pictures that have reinvigorated the romance genre . \",\n 'those so-so films that could have been much better ',\n 'a passable date film ',\n 'imax in short ',\n 'i hate it . ',\n \"'s pleasant enough -- and oozing with attractive men \",\n 'no one involved , save dash , shows the slightest aptitude for acting ',\n 'in this wildly uneven movie ',\n 'pokes , provokes , takes expressionistic license ',\n \"like one of ( spears ' ) music videos in content -- except that it goes on for at least 90 more minutes and , worse , that you have to pay if you want to see it \",\n 'with color and depth , and rather a good time ',\n 'silly , outrageous , ingenious ',\n \"being able to hit on a 15-year old when you 're over 100 \",\n \"the story of trouble every day ... is so sketchy it amounts to little more than preliminary notes for a science-fiction horror film , and the movie 's fragmentary narrative style makes piecing the story together frustrating difficult . \",\n \"the year 's best and most unpredictable comedy \",\n 'starts off so bad that you feel like running out screaming ',\n \"men in black ii achieves ultimate insignificance -- it 's the sci-fi comedy spectacle as whiffle-ball epic . \",\n 'elvira fans could hardly ask for more . ',\n 'a good one ',\n ', plodding picture ',\n 'the character dramas , which never reach satisfying conclusions ',\n 'lost in the translation this time ',\n 'speak for it while it forces you to ponder anew what a movie can be ',\n 'enables shafer to navigate spaces both large ... and small ... with considerable aplomb ',\n 'a plot cobbled together from largely flat and uncreative moments ',\n 'inane and awful ',\n 'told in scattered fashion ',\n 'takes chances that are bold by studio standards ',\n 'believe it or not , jason actually takes a backseat in his own film to special effects ',\n \"only thing to fear about `` fear dot com '' \",\n 'whole mess ',\n 'manages to infuse the rocky path to sibling reconciliation with flashes of warmth and gentle humor ',\n \"working from a surprisingly sensitive script co-written by gianni romoli ... ozpetek avoids most of the pitfalls you 'd expect in such a potentially sudsy set-up . \",\n 'a résumé loaded with credits like `` girl in bar # 3 ',\n 'enjoy the ride ',\n 'very little to add beyond the dark visions already relayed by superb recent predecessors ',\n 'the , yes , snail-like pacing ',\n 'an extremely unpleasant film . ',\n \"if it is n't entirely persuasive , it does give exposure to some talented performers \",\n 'the whole affair , true story or not , feels incredibly hokey ... ',\n \"are all things we 've seen before \",\n 'poignant and leavened ',\n 'breathless anticipation ',\n 'significantly better ',\n 'as predictable as the tides ',\n 'bewilderingly brilliant and entertaining ',\n \"it 's smooth and professional \",\n ', unassuming , subordinate ',\n 'the horror ',\n 'a technically superb film ',\n 'the positive change in tone here seems to have recharged him . ',\n 'pays earnest homage to turntablists and beat jugglers , old schoolers and current innovators ',\n 'a dim ',\n ', vicious and absurd ',\n 'because he acts so goofy all the time ',\n 'to a kinetic life so teeming that even cranky adults may rediscover the quivering kid inside ',\n \"'s at once laughable and compulsively watchable , \",\n 'merit its 103-minute length ',\n 'the value and respect for the term epic cinema ',\n \"as a dentist 's waiting room \",\n 'showing honest emotions ',\n 'amazingly lame . ',\n 'confessions may not be a straightforward bio ',\n 'seems a disappointingly thin slice of lower-class london life ; despite the title ... amounts to surprisingly little ',\n 'the story is bogus and ',\n 'this comic gem is as delightful as it is derivative . ',\n 'to spare wildlife ',\n 'ugly as the shabby digital photography ',\n 'understand the difference between dumb fun and just plain dumb ',\n 'traditionally structured ',\n \"frida is n't that much different from many a hollywood romance . \",\n 'does point the way for adventurous indian filmmakers toward a crossover into nonethnic markets . ',\n \"are worth the price of admission ... if `` gory mayhem '' is your idea of a good time \",\n 'was produced by jerry bruckheimer and directed by joel schumacher , and reflects the worst of their shallow styles : wildly overproduced , inadequately motivated every step of the way and demographically targeted to please every one ( and no one ) ',\n 'carnage and ',\n 'degraded , handheld blair witch video-cam footage ',\n 'second fiddle ',\n 'about the folly of superficiality that is itself ',\n 'serves as auto-critique , and its clumsiness as its own most damning censure . ',\n 'tedious norwegian offering which somehow snagged an oscar nomination . ',\n ', esther kahn is unusual but unfortunately also irritating . ',\n 'a stylish exercise ',\n 'halfway through this picture i was beginning to hate it ',\n 'a setup so easy it borders on facile ',\n 'a thoroughly awful movie ',\n 'takes hold and grips hard ',\n 'look smeary and blurry , to the point of distraction ',\n \"the picture 's fascinating byways \",\n 'guns , cheatfully filmed martial arts , disintegrating bloodsucker computer effects and jagged camera moves ',\n 'breezy , distracted rhythms ',\n 'than this mess ',\n 'evanescent , seamless and sumptuous stream ',\n 'slick and manufactured to claim street credibility . ',\n 'take care is nicely performed by a quintet of actresses , ',\n 'valuable messages ',\n 'in no small part thanks to lau ',\n 'two guys who desperately want to be quentin tarantino when they grow up ',\n 'no new plot conceptions ',\n 'essentially a collection of bits -- ',\n \"does n't bode well for the rest of it \",\n 'elevated by it -- the kind of movie ',\n 'usual worst ',\n 'hugh grant , who has a good line in charm ',\n 'arrive early and stay late ',\n 'is a pan-american movie , with moments of genuine insight into the urban heart . ',\n 'i could have used my two hours better watching being john malkovich again . ',\n 'shot in artful , watery tones of blue , green and brown ',\n 'an unorthodox little film noir organized crime story that includes one of the strangest ',\n 'considerable aplomb ',\n 'acted by diane lane and richard gere . ',\n 'scant ',\n 'an admittedly middling film ',\n 'too ludicrous ',\n \"the rock 's fighting skills are more in line with steven seagal \",\n 'rich and full ',\n 'warm water under a red bridge is a celebration of feminine energy , a tribute to the power of women to heal . ',\n 'that uses a sensational , real-life 19th-century crime as a metaphor for ',\n ', home movie will leave you wanting more , not to mention leaving you with some laughs and a smile on your face . ',\n ', the film retains ambiguities that make it well worth watching . ',\n 'ultra-cheesy dialogue ',\n 'added depth and resonance ',\n 'the sensational ',\n 'huge-screen format to make an old-fashioned nature film that educates viewers with words and pictures while entertaining them ',\n 'handsome but unfulfilling suspense drama ',\n \"'s not very good either . \",\n 'in her most charmless ',\n 'makes an unusual but pleasantly haunting debut behind the camera . ',\n 'a fine , rousing , g-rated family film , aimed mainly at little kids but with plenty of entertainment value to keep grown-ups from squirming in their seats . ',\n \"everything about girls ca n't swim , even its passages of sensitive observation , feels secondhand , familiar -- and not in a good way . \",\n 'the cast is spot on and ',\n 'ugly digital video ',\n 'can only love the players it brings to the fore for the gifted but no-nonsense human beings they are and for the still-inestimable contribution they have made to our shared history ',\n 'rather choppy ',\n 'becomes one more dumb high school comedy about sex gags and prom dates ',\n 'demands and receives excellent performances ',\n 'xxx is a blast of adrenalin , ',\n 'pure escapism ',\n \"create a film that 's not merely about kicking undead *** \",\n 'about love and culture ',\n 'is worse : ',\n 'horrifying ',\n 'mourns her tragedies in private ',\n 'the action scenes are poorly delivered ',\n \"it 's so bad \",\n 'so bad ',\n ', 84 minutes is short ',\n 'dabbles all around , never gaining much momentum ',\n 'a masterpiece ',\n 'a lot to do with the casting of juliette binoche as sand , who brings to the role her pale , dark beauty and characteristic warmth ',\n \"does n't leave you with much . \",\n 'good acting ',\n \"like most of jaglom 's films , some of it is honestly affecting \",\n \"marivaux 's rhythms , and mira sorvino 's limitations as a classical actress \",\n 'is less successful on other levels ',\n \"'s still quite worth seeing \",\n 'intelligent and moving . ',\n 'he appears miserable throughout as he swaggers through his scenes ',\n 'men ',\n 'accomplish what few sequels can ',\n 'a clunky tv-movie approach to detailing a chapter in the life of the celebrated irish playwright , poet and drinker ',\n \"i 'm not sure which half of dragonfly is worse : the part where nothing 's happening , or the part where something 's happening \",\n 'exhilarating , funny and fun . ',\n 'bound to appeal to women looking for a howlingly trashy time ',\n ', it never quite makes the grade as tawdry trash . ',\n 'has the right stuff for silly summer entertainment and has enough laughs to sustain interest to the end . ',\n 'endearing , caring ',\n 'seagal , who looks more like danny aiello these days ',\n 'enactments , however fascinating they may be as history , are too crude to serve the work especially well . ',\n 'not everything in this ambitious comic escapade works , but coppola , along with his sister , sofia , is a real filmmaker ',\n 'the dehumanizing and ego-destroying process ',\n 'taboo subject matter ',\n 'see this ',\n 'utterly incompetent conclusion ',\n \"has always been part of for the most part wilde 's droll whimsy helps `` being earnest '' overcome its weaknesses and parker 's creative interference ... \",\n \"you find yourself rooting for gai 's character to avoid the fate that has befallen every other carmen before her \",\n 'run for cover ',\n 'low budget ',\n 'a deceptively casual ode ',\n \"walked out muttering words like `` horrible '' and `` terrible , '' \",\n 'help chicago make the transition from stage to screen with considerable appeal intact ',\n 'you have ellen pompeo sitting next to you for the ride ',\n 'a giant step backward ',\n 'true fans ',\n 'just copies ',\n 'quite rich and exciting ',\n 'luminous interviews and amazingly evocative film from three decades ago ',\n 'from lynch , jeunet , and von trier while failing to find a spark of its own ',\n \", you ca n't help but get caught up in the thrill of the company 's astonishing growth . \",\n 'this frozen tundra soap opera ',\n 'a showdown ',\n 'is simply too overdone ',\n 'greatly ',\n 'free ',\n 'intriguing species ',\n 'try and evade ',\n 'scherfig , who has had a successful career in tv ',\n 'watching it leaves you giddy ',\n \"beautifully acted and directed , it 's clear that washington most certainly has a new career ahead of him if he so chooses . \",\n 'far tamer ',\n 'outrageously creative action ',\n 'the most purely enjoyable and satisfying evenings ',\n 'is never any question of how things will turn out ',\n 'only open new wounds ',\n 'despite suffering a sense-of-humour failure ',\n 'for a story ',\n \"in addition to hoffman 's powerful acting clinic \",\n 'right stuff ',\n 'the wonderful cinematography and naturalistic acting ',\n 'high-concept films ',\n 'sleepwalk through vulgarities in a sequel you can refuse ',\n 'with no reason for being ',\n 'just about all of the film is confusing on one level or another , making ararat far more demanding than it needs to be . ',\n 'invigorating , surreal , and resonant ',\n 'the chateau , a sense of light-heartedness , that makes it attractive throughout ',\n 'have a passion for the material ',\n \"maybe it is formula filmmaking , but there 's nothing wrong with that if the film is well-crafted and this one is . \",\n \"a movie that ca n't get sufficient distance from leroy 's \",\n 'romanticized rendering ',\n 'is terrific as rachel ',\n 'of special effects that run the gamut from cheesy to cheesier to cheesiest ',\n 'the worst film a man has made about women since valley of the dolls ',\n \"come , already having been recycled more times than i 'd care to count \",\n 'with awe ',\n 'joyous romp of a film . ',\n 'a nonstop hoot ',\n ', partisans and sabotage ',\n 'gory as the scenes of torture and self-mutilation ',\n 'just as the recent argentine film son of the bride reminded us that a feel-good movie can still show real heart ',\n \"ub equally spoofs and celebrates the more outre aspects of ` black culture ' and the dorkier aspects of ` white culture , ' even as it points out how inseparable the two are . \",\n 'the package in which this fascinating -- and timely -- content comes wrapped is disappointingly generic . ',\n 'this obscenely bad dark comedy , so crass ',\n 'naomi watts is terrific as rachel ; her petite frame and vulnerable persona emphasising her plight and isolation ',\n 'exciting and ',\n 'schticky ',\n 'and in the best way ',\n 'of our most flamboyant female comics ',\n 'squareness that would make it the darling of many a kids-and-family-oriented cable channel ',\n '( less a movie than ) an appalling , odoriferous thing ... so ',\n 'this film special ',\n 'of the amazing spider-man ',\n 'the stomach-turning violence ',\n 'could use a little more humanity ',\n 'not everything ',\n 'acquires an undeniable entertainment value as the slight , pale mr. broomfield continues to force himself on people and into situations that would make lesser men run for cover . ',\n 'a slow-moving police-procedural thriller ',\n 'a bad idea from frame one ',\n 'terrific , sweaty-palmed fun ',\n 'very funny joke ',\n \"volletta wallace 's maternal fury \",\n 'complex story ',\n 'atop wooden dialogue ',\n 'that it progresses in such a low-key manner that it risks monotony ',\n \"be burns 's strongest film since the brothers mcmullen \",\n \"ca n't miss it \",\n \"dover kosashvili 's outstanding feature debut so potent \",\n 'of the best short story writing ',\n 'it seeks excitement in manufactured high drama ',\n 'feeble comedy . ',\n 'an insult to every family ',\n 'it is essentially empty ',\n \"of the film 's problems \",\n 'makes my big fat greek wedding look like an apartheid drama ',\n 'is not even half the interest . ',\n 'never achieve the popularity of my big fat greek wedding ',\n \"`` auto focus '' works as an unusual biopic and document of male swingers in the playboy era \",\n \"right now , they 're merely signposts marking the slow , lingering death of imagination . \",\n 'appealing character quirks to forgive that still serious problem ',\n 'often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? ',\n 'the energy it takes to describe how bad it is ',\n 'wait to see it then . ',\n 'could easily be called the best korean film of 2002 ',\n 'largest-ever historical canvas ',\n 'a solid cast , assured direction and complete lack of modern day irony . ',\n 'comatose ballerinas ',\n 'oh-so-important ',\n 'of the road , where the thematic ironies are too obvious and the sexual politics too smug ',\n 'formulaic equations ',\n \"'s a work that , with humor , warmth , and intelligence , captures a life interestingly lived \",\n 'deserve a passing grade ',\n \"you 're gon na like this movie . \",\n 'simply stupid , irrelevant ',\n 'weaves both the elements of the plot and a powerfully evocative mood combining heated sexuality with a haunting sense of malaise . ',\n 'a heavy-handed indictment of parental failings and ',\n 'felt performances across the board . ',\n 'seem weird and distanced ',\n \"has finally made a movie that is n't just offensive \",\n 'casual and fun ',\n 'plodding mess ',\n \"the weird thing about the santa clause 2 , purportedly a children 's movie , is that there is nothing in it to engage children emotionally . \",\n \"the film 's intimate camera work \",\n \"you 'll find yourself wishing that you and they were in another movie . \",\n 'of dishonesty ',\n 'as bestial ',\n 'as it is a loose collection of not-so-funny gags , scattered moments of lazy humor ',\n ', they prove more distressing than suspenseful . ',\n 'crackles ',\n 'a fairly revealing study of its two main characters ',\n 'high drama , disney-style ',\n 'may be more genial than ingenious , but it gets the job done . ',\n ', good action , good acting , good dialogue , good pace , good cinematography . ',\n 'her bully of a husband ',\n 'gaghan ... has thrown every suspenseful cliché in the book at this nonsensical story . ',\n 'the darker elements of misogyny and unprovoked violence suffocate the illumination created by the two daughters ',\n 'oozing , chilling and heart-warming ',\n 'stale retread ',\n 'filming the teeming life on the reefs ',\n 'an already thin story boils down to surviving invaders seeking an existent anti-virus . ',\n 'quite endearing . ',\n 'with fewer gags to break the tedium ',\n 'overbearing and ',\n 'deep deceptions ',\n 'things will turn out okay ',\n 'the paranoid claustrophobia of a submarine movie with the unsettling spookiness of the supernatural ',\n 'grand ',\n 'good-natured fun found in films like tremors ',\n 'betters it ',\n 'transfixes the audience ',\n 'moving and important ',\n 'dope ',\n 'an avid interest ',\n 'blessed with two fine , nuanced lead performances ',\n 'true star ',\n 'hampered -- no , paralyzed -- by a self-indulgent script ... ',\n \"is expressly for idiots who do n't care what kind of sewage they shovel into their mental gullets to simulate sustenance . \",\n \"is haunting ... ( it 's ) what punk rock music used to be , and what the video medium could use more of : spirit , perception , conviction \",\n 'which nurses plot holes gaping enough to pilot an entire olympic swim team through ',\n \"that 's both charming and well acted \",\n 'a bad blend ',\n 'the guy-in-a-dress genre ',\n 'director claude chabrol has become the master of innuendo . ',\n 'riveting and ',\n 'excruciatingly unfunny and pitifully unromantic . ',\n 'war movies ',\n 'an inuit masterpiece ',\n 'oscar ',\n 'the cliché-laden screenplay ',\n 'girlfriends are bad , wives are worse ',\n 'provide much more insight ',\n \", and as easy to be bored by as your abc 's , \",\n 'what you wish for ',\n 'fluid and mesmerizing ',\n \"... a vivid , thoughtful , unapologetically raw coming-of-age tale full of sex , drugs and rock 'n' roll . \",\n \"it 's a testament to the film 's considerable charm that it succeeds in entertaining , despite playing out like a feature-length sitcom replete with stereotypical familial quandaries . \",\n 'ransacks ',\n 'lacks the charisma and ability to carry the film on his admittedly broad shoulders . ',\n 'book report ',\n 'leave the theater ',\n 'spousal abuse ',\n 'the director , with his fake backdrops and stately pacing , ',\n 'a diverse and astonishingly articulate cast of palestinian and israeli children . ',\n 'anyone who suffers through this film ',\n 'one carried by a strong sense of humanism ',\n 'for all its brilliant touches , dragon loses its fire midway , nearly flickering out by its perfunctory conclusion . ',\n 'another entertaining romp from robert rodriguez . ',\n \"directed with purpose and finesse by england 's roger mitchell , who handily makes the move from pleasing , relatively lightweight commercial fare such as notting hill to commercial fare with real thematic heft . \",\n \"it 'll probably be in video stores by christmas \",\n 'funny , triumphant , ',\n 'underachiever ',\n 'a film of ideas and wry comic mayhem ',\n 'of moviegoers for real characters and compelling plots ',\n 'dime-store ruminations ',\n 'worth a ',\n \", reality shows -- reality shows for god 's sake ! \",\n \"whatever the movie 's sentimental , hypocritical lessons about sexism , its true colors come out in various wet t-shirt and shower scenes . \",\n \"'ll cry for your money back \",\n 'enthralling ',\n \"is worse : the part where nothing 's happening , or the part where something 's happening \",\n \"like max rothman 's future , does not work . \",\n 'of quirky characters and an engaging story ',\n 'has the thrown-together feel of a summer-camp talent show : hastily written , underrehearsed , arbitrarily plotted and ',\n \"making kahlo 's art a living , \",\n 'bad run ',\n 'may just end up trying to drown yourself in a lake afterwards . ',\n 'many pleasures ',\n 'is worth searching out ',\n 'at this time , with this cast , this movie is hopeless ',\n 'time bombs ',\n 'balances both traditional or modern stories together in a manner that one never overwhelms the other . ',\n 'the surface histrionics failing to compensate for the paper-thin characterizations and facile situations ',\n 'plumbs uncharted depths of stupidity , incoherence and sub-sophomoric sexual banter . ',\n \"a sour taste in one 's mouth \",\n 'decent performance ',\n '... familiar and predictable , and 4/5ths of it might as well have come from a xerox machine rather than ( writer-director ) franc . ',\n \"a headline-fresh thriller set among orthodox jews on the west bank , joseph cedar 's time of favor manages not only to find a compelling dramatic means of addressing a complex situation , it does so without compromising that complexity . \",\n 'responsible for one of the worst movies of one year ',\n 'appalling , shamelessly manipulative and contrived ',\n 'movie love ',\n 'human darkness ',\n 'understand that the idea of exploiting molestation for laughs is funny , not actually exploiting it yourself ',\n \"it 's too loud to shout insults at the screen \",\n \"'s a long way from orwell 's dark , intelligent warning cry ( 1984 ) \",\n 'a coming-of-age film that avoids the cartoonish clichés and sneering humor of the genre as it provides a fresh view of an old type -- ',\n 'the perfect movie ',\n \"ardently waste viewers ' time with a gobbler like this \",\n 'oscar-sweeping franchise predecessor ',\n 'it sets out with no pretensions and delivers big time ',\n \"see how many times they can work the words `` radical '' or `` suck '' into a sentence \",\n 'give you enough to feel good about ',\n 'unexpectedly insightful ',\n \"allen 's romantic comedies so pertinent and enduring \",\n 'is an extraordinary film , not least ',\n 'a doubt ',\n 'maudlin or ',\n 'with the the wisdom and humor of its subjects ',\n 'a party-hearty teen flick that scalds like acid . ',\n 'the wan , thinly sketched story ',\n 'provide the funniest moments in this oddly sweet comedy about jokester highway patrolmen ',\n 'this romantic/comedy asks the question how much souvlaki can you take before indigestion sets in . ',\n 'again dazzle and delight us ',\n 'that beneath the familiar , funny surface is a far bigger , far more meaningful story than one in which little green men come to earth for harvesting purposes ',\n \"aspires to be more than another `` best man '' clone by weaving a theme throughout this funny film \",\n \"impresses as a skillfully assembled , highly polished and professional adaptation ... just about as chilling and unsettling as ` manhunter ' was . \",\n 'failure to construct a story with even a trace of dramatic interest ',\n \"but it also comes with the laziness and arrogance of a thing that already knows it 's won . \",\n \"it 's a frightful vanity film that , no doubt , pays off what debt miramax felt they owed to benigni . \",\n 'laughed at ',\n ', future lizard endeavors will need to adhere more closely to the laws of laughter ',\n 'a philosophical void ',\n 'unlikable characters and a self-conscious sense ',\n 'intelligence or invention ',\n 'hot on the hardwood proves once again that a man in drag is not in and of himself funny . ',\n 'hopelessly juvenile ',\n 'shakespearean -- both in depth and breadth -- ',\n 'unique directing style ',\n 'required to give this comic slugfest some heart ',\n 'feel more like a non-stop cry for attention ',\n 'hip hop beat ',\n 'feel more like literary conceits than flesh-and-blood humans ',\n 'no explanation ',\n 'funniest and ',\n 'a chilly , remote , emotionally distant piece ... so dull that its tagline should be ',\n 'starts off witty and sophisticated and you ',\n 'a bargain-basement european pickup ',\n \", but what really sets the film apart is debrauwer 's refusal to push the easy emotional buttons \",\n 'shines on all the characters , as the direction is intelligently accomplished ',\n 'taps into the primal fears of young people trying to cope with the mysterious and brutal nature of adults ',\n 'lead a group of talented friends astray ',\n \"it 's just weirdness for the sake of weirdness , and where human nature should be ingratiating , it 's just grating . \",\n 'this gentle , mesmerizing portrait ',\n 'genial is the conceit , this is one of those rare pictures that you root for throughout , ',\n 'draws the audience into the unexplainable pain and eccentricities that are attached to the concept of loss ',\n 'spliced together bits and pieces of midnight run and 48 hours ( and , for that matter , shrek ) ',\n 'is so busy making reference to other films and trying to be other films that it fails to have a heart , mind or humor of its own ',\n \"it lacks the compassion , good-natured humor and the level of insight that made ( eyre 's ) first film something of a sleeper success . \",\n 'handles it in the most unexpected way ',\n 'those eternally devoted to the insanity of black will have an intermittently good time . ',\n 'obscenely bad ',\n 'devastatingly ',\n \"i do n't see the point \",\n 'comes off like a bad imitation of the bard ',\n 'that excites the imagination and tickles the funny bone ',\n 'one of its strengths ',\n 'of masterpiece ',\n 'into this dream hispanic role with a teeth-clenching gusto ',\n 'stiff or just plain bad ',\n 'makes these lives count ',\n 'jolts the laughs from the audience -- ',\n 'a delightful stimulus ',\n 'has no point ',\n 'of isolation and frustration ',\n 'winning shot ',\n 'of its music or comic antics , but through the perverse pleasure of watching disney scrape the bottom of its own cracker barrel ',\n 'feeling guilty for it ... then , miracle of miracles , the movie does a flip-flop . ',\n 'lewis ',\n \"that there 's no other reason why anyone should bother remembering it \",\n \"bogdanovich puts history in perspective and , via kirsten dunst 's remarkable performance , he showcases davies as a young woman of great charm , generosity and diplomacy . \",\n 'father cliché ',\n 'domestic tension and unhappiness ',\n \"the effort is sincere and the results are honest , but the film is so bleak that it 's hardly watchable \",\n 'called the best korean film of 2002 ',\n 'one of the best rock ',\n 'it forces you to watch people doing unpleasant things to each other and themselves ',\n 'of the holiday box office pie ',\n 'inexpressible and drab wannabe ',\n 'the hail of bullets , none of which ever seem to hit ',\n 'like an extended , open-ended poem than a traditionally structured story ',\n \"'s about as convincing as any other arnie musclefest , but has a little too much resonance with real world events and \",\n 'to hate ',\n 'to be mesmerised ',\n 'the people who loved the 1989 paradiso will prefer this new version ',\n 'a lot of energy ',\n 'for its success on a patient viewer ',\n 'while tattoo borrows heavily from both seven and the silence of the lambs , it manages to maintain both a level of sophisticated intrigue and human-scale characters that suck the audience in . ',\n \"this may be burns 's strongest film since the brothers mcmullen . \",\n 'turned down ',\n 'timely , tongue-in-cheek ',\n 'undramatic ',\n 'the pile of useless actioners ',\n 'morton deserves an oscar nomination . ',\n 'weirdly engaging and unpredictable character pieces ',\n 'efficiency and an affection for the period ',\n 'manages to accomplish what few sequels can -- it equals the original and in some ways even betters it ',\n 'does not work . ',\n \"it is n't merely offensive \",\n 'tell a story about the vietnam war before the pathology set in ',\n 'it irrigates our souls . ',\n 'a hint of humor ',\n 'with a big heart ',\n 'a rollicking adventure for you and all your mateys , regardless of their ages ',\n 'its most immediate and most obvious pleasure ',\n \"there are laughs aplenty , and , as a bonus , viewers do n't have to worry about being subjected to farts , urine , feces , semen , or any of the other foul substances that have overrun modern-day comedies \",\n 'one of those rare films that seems as though it was written for no one , but somehow manages to convince almost everyone that it was put on the screen , just for them . ',\n \"made a decent ` intro ' documentary \",\n 'makes us see familiar issues , like racism and homophobia , in a fresh way . ',\n 'the film goes right over the edge and kills every sense of believability ',\n \"a beyond-lame satire , teddy bears ' picnic ranks among the most pitiful directing debuts by an esteemed writer-actor . \",\n 'emotional seesawing ',\n \"movies with the courage to go over the top and movies that do n't care about being stupid \",\n \"target audience has n't graduated from junior high school \",\n 'this intricately structured and well-realized drama ',\n 'embracing ',\n 'the film a celluloid litmus test for the intellectual and emotional pedigree of your date and a giant step backward for a director i admire ',\n 'a meditation on faith and madness , frailty is blood-curdling stuff . ',\n 'a pretty listless collection ',\n 'other than its oscar-sweeping franchise predecessor ',\n \"almost everyone growing up believes their family must look like `` the addams family '' to everyone looking in \",\n 'have been called freddy gets molested by a dog ',\n 'moody male hustler ',\n 'uselessly redundant and shamelessly money-grubbing than most third-rate horror sequels ',\n 'the film is filled with humorous observations about the general absurdity of modern life as seen through the eyes outsiders , but deftly manages to avoid many of the condescending stereotypes that so often plague films dealing with the mentally ill ',\n 'which is mostly a bore ',\n 'an entertaining mix of period drama and flat-out farce that should please history fans ',\n \"suspect that you 'll be as bored watching morvern callar as the characters are in it . \",\n 'the problems and characters it reveals are universal and involving , and the film itself -- as well its delightful cast -- is so breezy , pretty and gifted , it really won my heart . ',\n 'love this movie ',\n '... the efforts of its star , kline , to lend some dignity to a dumb story are for naught . ',\n 'flat as the scruffy sands of its titular community ',\n 'liberal doses of dark humor , gorgeous exterior photography , and a stable-full of solid performances ',\n 'is badly edited , often awkwardly directed and suffers from the addition of a wholly unnecessary pre-credit sequence designed to give some of the characters a ` back story ',\n 'works smoothly ',\n 'with a creepy and dead-on performance ',\n \"might want to take a reality check before you pay the full ticket price to see `` simone , '' and consider a dvd rental instead . \",\n 'placed in the pantheon of the best of the swashbucklers ',\n 'a satisfyingly unsettling ride ',\n 'good a job as anyone ',\n ', smart and complicated ',\n 'the intimate , unguarded moments of folks who live in unusual homes -- ',\n 'are pretty valuable these days ',\n \"( i ) t 's certainly laudable that the movie deals with hot-button issues in a comedic context , but barbershop is n't as funny as it should be . \",\n 'that the only rip off that we were aware of ',\n 'beautiful to watch and ',\n 'accentuating ',\n 'no quarter to anyone seeking to pull a cohesive story out of its 2 1/2 - hour running time ',\n 'a wonderful ensemble cast ',\n 'endear ',\n 'the truly funny bits ',\n \"'s that painful . \",\n 'american beauty reeks ',\n 'clotted with heavy-handed symbolism , dime-store psychology and endless scenic shots that make 105 minutes seem twice as long ',\n 'without any of its sense of fun or energy ',\n \"be the cat 's meow \",\n \"'s simply stupid , irrelevant and deeply \",\n 'wonderland adventure , a stalker thriller , and ',\n 'be part of ',\n 'cinema history as the only movie ever ',\n 'comes from a renowned indian film culture that allows americans to finally revel in its splendor ',\n 'accomplished oscar winners ',\n 'of those underrated professionals who deserve but rarely receive it ',\n 'we find ourselves surprised at how much we care about the story ',\n 'an all-time low ',\n 'enjoyed it just ',\n 'reacting to it - feeling a part of its grand locations , ',\n \"delivered with such conviction that it 's hard not to be carried away \",\n 'colorful , vibrant introduction ',\n 'the slow parade of human frailty fascinates you ',\n \"famuyiwa 's feature deals with its subject matter in a tasteful , intelligent manner , rather than forcing us to endure every plot contrivance that the cliché-riddled genre can offer . \",\n 'a laugh between them ',\n 'that gradually sneaks up on the audience ',\n 'photographic marvel ',\n 'is shockingly bad and absolutely unnecessary . ',\n 'gets vivid performances from her cast and pulls off some deft ally mcbeal-style fantasy sequences . ',\n \"the result , however well-intentioned , is ironically just the sort of disposable , kitchen-sink homage that illustrates why the whole is so often less than the sum of its parts in today 's hollywood . \",\n 'while not for every taste , this often very funny collegiate gross-out comedy goes a long way toward restoring the luster of the national lampoon film franchise , too long reduced to direct-to-video irrelevancy . ',\n \"wow ' factor \",\n 'hashiguchi covers this territory with wit and originality , suggesting that with his fourth feature -- the first to be released in the u.s. -- a major director is emerging in world cinema . ',\n 'brazil-like , hyper-real satire ',\n 'low on both suspense and payoff ',\n 'find a scathing portrayal of a powerful entity strangling the life out of the people who want to believe in it the most ',\n 'an absolute delight for all audiences ',\n \"would tax einstein 's brain . \",\n \"'s its first sign of trouble \",\n \"ca n't believe anyone would really buy this stuff \",\n 'moonlight mile should strike a nerve in many . ',\n 'if ever such a dependable concept was botched in execution ',\n 'wedding feels a bit anachronistic . ',\n 'been lost in the translation this time ',\n 'revelatory performance ',\n 'the pianist like a surgeon mends a broken heart ; very meticulously but without any passion ',\n 'remains oddly detached ',\n 'that is more complex and honest than anything represented in a hollywood film ',\n 'nifty premise ',\n 'vein ',\n 'the wonderfully lush morvern callar is pure punk existentialism , and ',\n 'go down with a ship as leaky ',\n 'earnest movie ',\n 'effective enough ',\n \"showing us well-thought stunts or a car chase that we have n't seen 10,000 times \",\n 'make the film more silly than scary , like some sort of martha stewart decorating program run amok ',\n 'going at a rapid pace , ',\n 'still fun and enjoyable and ',\n 'a lazy exercise in bad filmmaking ',\n 'this thing is virtually unwatchable . ',\n \"'s also undeniably exceedingly clever \",\n 'would ever work in a mcculloch production again if they looked at how this movie turned out ',\n 'an entertainment destination for the general public ',\n 'i have ever seen , constantly pulling the rug from underneath us , seeing things from new sides , plunging deeper , getting more intense . ',\n \"england 's roger mitchell , who handily makes the move from pleasing \",\n 'to the enduring strengths of women ',\n 'surprisingly faithful ',\n 'the numerous scenes ',\n 'exaggerated and ',\n 'takes a really long , slow and dreary time ',\n 'improved upon the first ',\n 'that might have made it an exhilarating ',\n 'k-19 : the widowmaker is a great yarn . ',\n 'humorless ',\n 'thank ',\n 'just because a walk to remember ',\n 'bite ',\n 'amounts to little more than punishment . ',\n 'the humor is recognizably plympton ',\n 'likeable thanks to its cast , its cuisine and its quirky tunes . ',\n 'makes piecing the story together frustrating difficult ',\n 'a fascinating , compelling story ',\n \"the film 's hero is a bore and his innocence soon becomes a questionable kind of inexcusable dumb innocence \",\n 'a fascinating , dark thriller that keeps you hooked on the delicious pulpiness of its lurid fiction . ',\n 'whimsical and relevant today ',\n 'see the it ',\n 'tackles the difficult subject of grief and loss ',\n 'grotesque narcissism ',\n 'funny and sad ',\n \"is , arguably , the most accomplished work to date from hong kong 's versatile stanley kwan . \",\n 'it wore me down ',\n 'be so stupid ',\n 'as refreshing ',\n 'what we get in feardotcom is more like something from a bad clive barker movie . ',\n 'has created a brilliant motion picture . ',\n 'from the clumsy cliché of the ugly american ',\n 'a beautiful food entrée ',\n \"us a slice of life that 's very different from our own and yet instantly recognizable \",\n 'the passive-aggressive psychology of co-dependence ',\n \"his story ends or just ca n't tear himself away from the characters \",\n 'does not live up to its style ',\n 'scenes of cinematic perfection ',\n \"love liza 's tale \",\n \"the type of film about growing up that we do n't see often enough these days : realistic , urgent \",\n \"first , for a movie that tries to be smart , it 's kinda dumb . \",\n 'go see this delightful comedy . ',\n 'worst sense ',\n 'propelled by the acting ',\n 'presents us with an action movie that actually has a brain . ',\n 'human nature is a goofball movie , in the way that malkovich was , but it tries too hard ',\n 'overplayed ',\n 'should be poignant ',\n \"'' feels capable of charming the masses with star power , a pop-induced score and sentimental moments that have become a spielberg trademark . \",\n 'visceral and dangerously honest revelations about the men and machines behind the curtains of our planet ',\n 'kirshner and monroe seem to be in a contest to see who can out-bad-act the other . ',\n 'mug shots ',\n 'the barbarism of ` ethnic cleansing ',\n 'another masterpiece ',\n 'an incredibly layered and stylistic film ',\n 'a classic story ',\n 'daring ',\n \"wo n't feel cheated by the high infidelity of unfaithful . \",\n 'haute ',\n \"convey a strong sense of the girls ' environment . \",\n 'ugly and ',\n 'a journey spanning nearly three decades of bittersweet camaraderie and history , in which we feel that we truly know what makes holly and marina tick ',\n 'any insight ',\n \"a well-intentioned effort that 's still too burdened by the actor 's offbeat sensibilities for the earnest emotional core to emerge with any degree of accessibility . \",\n 'passionate , tumultuous affair ',\n 'the kind of under-inspired , overblown enterprise that gives hollywood sequels a bad name ',\n 'an enjoyable experience . ',\n \"'ve had more interesting -- and , dare i say , thematically complex -- bowel movements than this long-on-the-shelf , point-and-shoot exercise in gimmicky crime drama . \",\n 'is a greater attention to the parents -- and particularly the fateful fathers -- in the emotional evolution of the two bewitched adolescents . ',\n 'well to cram earplugs ',\n 'who is simply tired ',\n \"it 's the little nuances that perhaps had to escape from director mark romanek 's self-conscious scrutiny to happen , that finally get under your skin \",\n 'both convincing and radiant ',\n 'smart and dark - ',\n 'emaciated flick ',\n 'jim brown treats his women -- as dumb , credulous , unassuming , subordinate subjects ',\n 'point at things that explode into flame ',\n 'jolts the laughs from the audience ',\n 'heralds something special ',\n \"has n't escaped the rut dug by the last one \",\n 'a perfect performance ',\n 'this cinema verite speculation on the assassination of john f. kennedy may have been inspired by blair witch , but it takes its techniques into such fresh territory that the film never feels derivative ',\n 'an unsuccessful attempt at a movie ',\n 'sometimes confusing ',\n 'superlative ',\n 'buy the soundtrack ',\n \"'s sharply comic and surprisingly touching , \",\n 'a soap-opera quality twist in the last 20 minutes ... almost puts the kibosh on what is otherwise a sumptuous work of b-movie imagination . ',\n 'to change hackneyed concepts when it comes to dreaming up romantic comedies ',\n 'the folly of superficiality that is itself ',\n 'lumbering load ',\n 'a 95-minute commercial for nba properties ',\n 'though many of these guys are less than adorable ',\n 'peels layers from this character that may well not have existed on paper . ',\n 'is deadly dull ',\n 'inferior ',\n 'big-hearted and frequently ',\n 'striking deep chords of sadness ',\n 'laughs aplenty ',\n \"schaeffer is n't in this film , which may be why it works as well as it does . \",\n 'loud , ugly , irritating movie ',\n 'is impressive for the sights and sounds of the wondrous beats the world has to offer ',\n 'the scorpion king more than ably ',\n 'make a pretty good team ',\n 'lags badly in the middle and lurches between not-very-funny comedy , unconvincing dramatics and some last-minute action strongly reminiscent of run lola run ',\n 'does a great combination act as narrator , jewish grandmother and subject -- taking us through a film that is part biography , part entertainment and part history . ',\n 'bill plympton , the animation master ',\n 'impossible task ',\n 'that in a good way ',\n 'a watch that makes time go faster rather than ',\n 'very best pictures ',\n \"lack their idol 's energy and passion for detail \",\n 'and church meetings ',\n 'than in creating an emotionally complex , dramatically satisfying heroine ',\n 'compassionate ',\n \"will find in these characters ' foibles a timeless and unique perspective \",\n 'hugely entertaining from start to finish , featuring a fall from grace that still leaves shockwaves ',\n 'an ebullient affection ',\n 'somehow snagged an oscar nomination ',\n 'manages to show the gentle and humane side of middle eastern world politics ',\n \"` realistic ' \",\n 'nicely done ',\n 'a summer of good stuff ',\n 'the satire is just too easy to be genuinely satisfying . ',\n 'sustains ',\n 'neither is it as smart ',\n 'fare , with enough creative energy and wit to entertain all ages ',\n 'the movie is powerful and provocative . ',\n \"time literally stops on a dime in the tries-so-hard-to-be-cool `` clockstoppers , '' but that does n't mean it still wo n't feel like the longest 90 minutes of your movie-going life \",\n 'sharp edges and a deep vein of sadness ',\n \"those who do n't entirely ` get ' godard 's distinctive discourse will still come away with a sense of his reserved but existential poignancy . \",\n \"not only are the film 's sopranos gags incredibly dated and unfunny , they also demonstrate how desperate the makers of this ` we 're - doing-it-for - the-cash ' sequel were . \",\n \"'n safe as to often play like a milquetoast movie of the week blown up for the big screen \",\n 'like to skip but film buffs should get to know ',\n 'narrative discipline ',\n 'samuel l. jackson ',\n 'is this films reason for being . ',\n \"the old saying goes , because it 's true \",\n 'take any 12-year-old boy to see this picture ',\n 'aims for poetry and ends up sounding like satire ',\n 'come close to the level of intelligence and visual splendor that can be seen in other films ',\n 'embracing than monty ',\n 'is it hokey ',\n \"as assaults on america 's knee-jerk moral sanctimony \",\n \"that 's hardly any fun to watch \",\n \"get caught up in the thrill of the company 's astonishing growth \",\n \"it 's on par with the first one \",\n 'is moody , oozing , chilling and heart-warming all at once ',\n 'is an accuracy of observation in the work of the director , frank novak , that keeps the film grounded in an undeniable social realism ',\n 'is in the right place , his plea for democracy and civic action laudable ',\n 'above most of its ilk ',\n 'with spy kids 2 : the island of lost dreams , however , robert rodriguez adorns his family-film plot with an elegance and maturity that even most contemporary adult movies are lacking . ',\n 'from kevin kline who unfortunately works with a two star script ',\n 'change watching such a character , especially when rendered in as flat and ',\n \"i do n't think so . \",\n 'devotees of star trek ii : the wrath of khan will feel a nagging sense of deja vu , and the grandeur of the best next generation episodes is lacking ',\n 'is always a joy to watch , even when her material is not first-rate ',\n 'great to see this turd squashed under a truck , preferably a semi ',\n 'bile , and irony ',\n 'a faulty premise , ',\n 'haphazardness ',\n 'the plot grows thin soon , ',\n 'beloved-major ',\n 'by no means a great movie , but it is a refreshingly forthright one . ',\n 'crossing-over mumbo jumbo ',\n 'virtually unwatchable ',\n 'its premise is smart , but ',\n \"was amused and entertained by the unfolding of bielinsky 's cleverly constructed scenario , and greatly impressed by the skill of the actors involved in the enterprise . \",\n \"who seem bound and determined to duplicate bela lugosi 's now-cliched vampire accent \",\n 'light-hearted ',\n 'heavy with flabby rolls of typical toback machinations . ',\n 'to show us a good time ',\n 'pitiful , slapdash disaster . ',\n \"is one of the year 's best \",\n 'even witty ',\n 'a fierce dance ',\n 'crippled ',\n 'this piece of crap ',\n '... the lady and the duke surprisingly manages never to grow boring ... which proves that rohmer still has a sense of his audience . ',\n 'the most wondrous love story in years , it is a great film . ',\n 'its story about a young chinese woman , ah na , who has come to new york city to replace past tragedy with the american dream ',\n 'particularly impressive ',\n 'in the process comes out looking like something wholly original ',\n 'submerging ',\n 'nearly as fresh or enjoyable ',\n 'miserable and smug ',\n 'my great pleasure ',\n 'it all unfolds predictably , and the adventures that happen along the way seem repetitive and designed to fill time , providing no real sense of suspense . ',\n 'fascinating , dark thriller ',\n \"more of the same from taiwanese auteur tsai ming-liang , which is good news to anyone who 's fallen under the sweet , melancholy spell of this unique director 's previous films . \",\n 'down the reality drain ',\n 'the only way to tolerate this insipid , brutally clueless film might be with a large dose of painkillers . ',\n 'a historic scandal ',\n \"that 's steeped in mystery and a ravishing , baroque beauty \",\n \"cool stuff packed into espn 's ultimate x. \",\n 'idea ( of middle-aged romance ) is not handled well and , except for the fine star performances ',\n 'their lamentations are pretty much self-centered ',\n 'a pretty decent kid-pleasing , ',\n 'of classic romantic comedy to which it aspires ',\n ', bigelow demonstrates a breadth of vision and an attention to detail that propels her into the upper echelons of the directing world . ',\n 'is the refreshingly unhibited enthusiasm that the people , in spite of clearly evident poverty and hardship , bring to their music ',\n \"would n't have taken the protagonists a full hour to determine that in order to kill a zombie you must shoot it in the head \",\n 'give a pretty good overall picture of the situation in laramie following the murder of matthew shepard ',\n 'the fine line between cheese and earnestness remarkably well ',\n 'a documentary fails to live up to -- or offer any new insight into -- its chosen topic ',\n 'serious movie-goers embarking upon this journey ',\n 'insistent and repetitive ',\n 'nausea ',\n 'of survival wrapped in the heart-pounding suspense of a stylish psychological thriller ',\n 'no art ',\n 'seems to exist only for its climactic setpiece ',\n 'more silly than scary ',\n \"'s funny . \",\n 'its execution and skill ',\n \"'s coherent , well shot , and tartly \",\n 'you wish you were at home watching that movie instead of in the theater watching this one ',\n \"'s no point in extracting the bare bones of byatt 's plot for purposes of bland hollywood romance \",\n 'underdeveloped ',\n 'the jokes are flat ',\n 'a heartening tale of small victories ',\n ...]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][\"sentence\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/68 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2b2f08654cc49bea05a25e85945c40e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\01149762\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-3b973809f4ba4741.arrow\n",
      "Loading cached processed dataset at C:\\Users\\01149762\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-0f17b6bcebe8fc01.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\",\"idx\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "small_train_dataset = tokenized_datasets[\"train\"]#.shuffle(seed=42).select(range(10000))\n",
    "small_eval_dataset = tokenized_datasets[\"validation\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bardzo mały batch size - bo model jest za duży."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=10)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = Adam(sentiment_model.parameters(), lr=5e-5)\n",
    "sentiment_model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Uczenie"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21367141640151877\n",
      "0.11424943384045144\n",
      "0.07635031398741837\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        batch = {\"attention_mask\": batch['attention_mask'].to(device), \"input_ids\": batch['input_ids'].to(device)}#, \"token_type_ids\":batch['token_type_ids'].to(device)}\n",
    "        outputs = sentiment_model(**batch)\n",
    "        loss = loss_fun(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        #         lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        # progress_bar.update(1)\n",
    "        losses.append(loss.item())\n",
    "    print(np.mean(losses))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "torch.save(sentiment_model.state_dict(),\"models/sentiment_model_dict\")\n",
    "# sentiment_model.load_state_dict(torch.load(\"sentiment_model_dict\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ewaluacja"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "{'accuracy': 0.8807339449541285}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric= load_metric(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    labels = batch[\"label\"].to(device)\n",
    "    batch = {\"attention_mask\": batch['attention_mask'].to(device), \"input_ids\": batch['input_ids'].to(device)}#, \"token_type_ids\":batch['token_type_ids'].to(device)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = sentiment_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=labels)\n",
    "\n",
    "metric.compute()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Odpowiadanie na pytania (Question answering)\n",
    "Embeddigni można wykorzystać do wielu zastosowań."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "context = '''Warsaw is the capital and largest city of Poland.\n",
    "The metropolis stands on the River Vistula in east-central Poland and its population is officially estimated at 1.8 million\n",
    "residents within a greater metropolitan area of 3.1 million residents, which makes Warsaw the 7th\n",
    "most-populous capital city in the European Union.\n",
    "The city area measures 517 km2 (200 sq mi) and comprises 18 boroughs,\n",
    "while the metropolitan area covers 6,100 km2 (2,355 sq mi).\n",
    "Warsaw is an alpha- global city, a major cultural, political and economic hub,\n",
    "and the country's seat of government. Its historical Old Town was designated a UNESCO World Heritage Site.'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "nlp = transformers.pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Where does the water in Warsaw come from?',\n",
    "    'context': context}\n",
    "res = nlp(QA_input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "{'score': 0.7980945110321045,\n 'start': 79,\n 'end': 92,\n 'answer': 'River Vistula'}"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}